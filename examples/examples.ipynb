{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanekercheval/repos/sik-llms/src\n"
     ]
    }
   ],
   "source": [
    "%cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.openai.OpenAI at 0x107ec9550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For \"registered\" clients (via `@Client.register`), the client\n",
    "# can be created with `create_client` by passing in the model name.\n",
    "from sik_llms import create_client\n",
    "\n",
    "client = create_client(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.openai.OpenAI at 0x10e2c1810>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, the client can be directly instantiated\n",
    "from sik_llms import OpenAI\n",
    "client = OpenAI(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.anthropic.Anthropic at 0x10e3c7e00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, the client can be directly instantiated\n",
    "from sik_llms import Anthropic\n",
    "client = Anthropic(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user', 'content': 'What is the capital of France?'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sik_llms import create_client, user_message, TextChunkEvent\n",
    "\n",
    "client = create_client(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the capital of France?\")\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Synchronously via `__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=14 output_tokens=8 input_cost=2.1e-06 output_cost=4.8e-06 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=1.123208999633789 response='The capital of France is Paris.'\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = client(messages=[message])\n",
    "print(response)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Aynchronously via `run_async`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=14 output_tokens=8 input_cost=2.1e-06 output_cost=4.8e-06 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=0.8110058307647705 response='The capital of France is Paris.'\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = await client.run_async(messages=[message])\n",
    "print(response)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Asynchronously via `stream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "async for response in client.stream(messages=[message]):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        print(response.content, end=\"\")\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextChunkEvent(content='The', logprob=None),\n",
       " TextChunkEvent(content=' capital', logprob=None),\n",
       " TextChunkEvent(content=' of', logprob=None),\n",
       " TextChunkEvent(content=' France', logprob=None),\n",
       " TextChunkEvent(content=' is', logprob=None),\n",
       " TextChunkEvent(content=' Paris', logprob=None),\n",
       " TextChunkEvent(content='.', logprob=None),\n",
       " TextResponse(input_tokens=14, output_tokens=8, input_cost=2.1e-06, output_cost=4.8e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.4176782909780741, response='The capital of France is Paris.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate multiple responses from a single input via `sample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['42', '47829', '74']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TextResponse(input_tokens=16, output_tokens=2, input_cost=2.4e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.5277789169922471, response='42'),\n",
       " TextResponse(input_tokens=16, output_tokens=3, input_cost=2.4e-06, output_cost=1.8e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.6055014161393046, response='47829'),\n",
       " TextResponse(input_tokens=16, output_tokens=2, input_cost=2.4e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.45302558317780495, response='74')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = create_client(model_name='gpt-4o-mini', temperature=1.5)\n",
    "responses = await client.sample(\n",
    "    messages=[user_message('Generate a random number. Return only the number')],\n",
    "    n=3,\n",
    ")\n",
    "print([r.response for r in responses])\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate multiple responses from multiple inputs via `generate_multiple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paris', 'Rome']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TextResponse(input_tokens=20, output_tokens=2, input_cost=3e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.8573106250260025, response='Paris'),\n",
       " TextResponse(input_tokens=20, output_tokens=2, input_cost=3e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.8546491248998791, response='Rome')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = create_client(model_name='gpt-4o-mini', temperature=1.5)\n",
    "responses = await client.generate_multiple(\n",
    "    messages=[\n",
    "        [user_message(\"What is the capital of France? Return only the city name.\")],\n",
    "        [user_message(\"What is the capital of Italy? Return only the city name.\")],\n",
    "    ],\n",
    ")\n",
    "print([r.response for r in responses])\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate `sample_n` responses from multiple inputs via `generate_multiple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['47', '43', '47', '42', '67']\n",
      "['137', '156', '154', '147', '153']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[TextResponse(input_tokens=23, output_tokens=2, input_cost=3.45e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=1.2576175420545042, response='47'),\n",
       "  TextResponse(input_tokens=23, output_tokens=2, input_cost=3.45e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.8492839590180665, response='43'),\n",
       "  TextResponse(input_tokens=23, output_tokens=2, input_cost=3.45e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.8483560001477599, response='47'),\n",
       "  TextResponse(input_tokens=23, output_tokens=2, input_cost=3.45e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=1.1479352081660181, response='42'),\n",
       "  TextResponse(input_tokens=23, output_tokens=2, input_cost=3.45e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=1.1884103750344366, response='67')],\n",
       " [TextResponse(input_tokens=23, output_tokens=2, input_cost=3.45e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.733713542111218, response='137'),\n",
       "  TextResponse(input_tokens=23, output_tokens=2, input_cost=3.45e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.6759564578533173, response='156'),\n",
       "  TextResponse(input_tokens=23, output_tokens=2, input_cost=3.45e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.7058872089255601, response='154'),\n",
       "  TextResponse(input_tokens=23, output_tokens=2, input_cost=3.45e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.6860343748703599, response='147'),\n",
       "  TextResponse(input_tokens=23, output_tokens=2, input_cost=3.45e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.7594079999253154, response='153')]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = create_client(model_name='gpt-4o-mini', temperature=1.5)\n",
    "responses_set = await client.generate_multiple(\n",
    "    messages=[\n",
    "        [user_message(\"Pick a random number between 1 and 100. Return only the number.\")],\n",
    "        [user_message(\"Pick a random number between 100 and 200. Return only the number.\")],\n",
    "    ],\n",
    "    sample_n=5,\n",
    ")\n",
    "for responses in responses_set:\n",
    "    print([r.response for r in responses])\n",
    "responses_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Functions/Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=60 output_tokens=17 input_cost=9e-06 output_cost=1.0199999999999999e-05 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=0.8125739097595215 tool_prediction=ToolPrediction(name='get_weather', arguments={'location': 'Paris, France'}, call_id='call_fiQ9qpvKIxfucsKHigT3nr2O') message=None\n",
      "---\n",
      "name='get_weather' arguments={'location': 'Paris, France'} call_id='call_fiQ9qpvKIxfucsKHigT3nr2O'\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    Tool, Parameter, RegisteredClients,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name='get_weather',\n",
    "    description=\"Get the weather for a location.\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='location',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description='The city and country for weather info.',\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    client_type=RegisteredClients.OPENAI_TOOLS,\n",
    "    model_name='gpt-4o-mini',\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the weather in Paris?\")\n",
    "response = await client.run_async(messages=[message])\n",
    "# or `response = client(messages=[message])` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.tool_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude Functions/Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=401 output_tokens=40 input_cost=0.001203 output_cost=0.0006000000000000001 cache_write_tokens=0 cache_read_tokens=0 cache_write_cost=0.0 cache_read_cost=0.0 duration_seconds=1.5602500438690186 tool_prediction=ToolPrediction(name='get_weather', arguments={'location': 'Paris, France'}, call_id='toolu_019xogKTwcVzGRxdVP3Xamof') message=None\n",
      "---\n",
      "name='get_weather' arguments={'location': 'Paris, France'} call_id='toolu_019xogKTwcVzGRxdVP3Xamof'\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    Tool, Parameter, RegisteredClients,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name='get_weather',\n",
    "    description=\"Get the weather for a location.\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='location',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description='The city and country for weather info.',\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    client_type=RegisteredClients.ANTHROPIC_TOOLS,\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the weather in Paris?\")\n",
    "response = await client.run_async(messages=[message])\n",
    "# or `response = client(messages=[message])` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.tool_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs via OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=92 output_tokens=18 input_cost=1.38e-05 output_cost=1.08e-05 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=0.8882229158189148 parsed=CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob']) refusal=None\n",
      "---\n",
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):  # noqa: D101\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "client = create_client(\n",
    "    model_name='gpt-4o-mini',\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "messages=[\n",
    "    system_message(\"Extract the event information.\"),\n",
    "    user_message(\"Alice and Bob are going to a science fair on Friday.\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "# or `response = client(messages=messages)` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs via Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=441 output_tokens=78 input_cost=0.001323 output_cost=0.00117 cache_write_tokens=None cache_read_tokens=None cache_write_cost=None cache_read_cost=None duration_seconds=2.373826026916504 parsed=CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob']) refusal=None\n",
      "---\n",
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):  # noqa: D101\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "client = create_client(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "messages=[\n",
    "    system_message(\"Extract the event information.\"),\n",
    "    user_message(\"Alice and Bob are going to a science fair on Friday.\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "# or `response = client(messages=messages)` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning via OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the expression 1 + 2 + (3 * 4) + (5 * 6), follow these steps:\n",
      "\n",
      "1. First, calculate the multiplication parts:\n",
      "   - 3 * 4 = 12\n",
      "   - 5 * 6 = 30\n",
      "\n",
      "2. Then, add the results along with the remaining numbers:\n",
      "   - 1 + 2 = 3\n",
      "   - Now, 3 + 12 = 15\n",
      "   - Finally, 15 + 30 = 45\n",
      "\n",
      "So, the final answer is 45."
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    TextChunkEvent, TextResponse, ReasoningEffort,\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    model_name='o3-mini',\n",
    "    reasoning_effort=ReasoningEffort.MEDIUM,\n",
    ")\n",
    "messages=[user_message(\"What is 1 + 2 + (3 * 4) + (5 * 6)?\")]\n",
    "summary = None\n",
    "async for response in client.stream(messages=messages):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        print(response.content, end=\"\")\n",
    "    elif isinstance(response, TextResponse):\n",
    "        summary = response\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response type: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextResponse(input_tokens=27, output_tokens=193, input_cost=2.97e-05, output_cost=0.0008492, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=3.037583827972412, response='To solve the expression 1 + 2 + (3 * 4) + (5 * 6), follow these steps:\\n\\n1. First, calculate the multiplication parts:\\n   - 3 * 4 = 12\\n   - 5 * 6 = 30\\n\\n2. Then, add the results along with the remaining numbers:\\n   - 1 + 2 = 3\\n   - Now, 3 + 12 = 15\\n   - Finally, 15 + 30 = 45\\n\\nSo, the final answer is 45.')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning via Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[THINKING]\n",
      "I need to calculate the value of the expression 1 + 2 + (3 * 4) + (5 * 6).\n",
      "\n",
      "Following the order of operations (PEMDAS), I need to do the multiplication first, then addition.\n",
      "\n",
      "(3 * 4) = 12\n",
      "(5 * 6) = 30\n",
      "\n",
      "Now I have:\n",
      "1 + 2 + 12 + 30\n",
      "\n",
      "Adding these up:\n",
      "1 + 2 = 3\n",
      "3 + 12 = 15\n",
      "15 + 30 = 45\n",
      "\n",
      "So the answer is 45.\n",
      "\n",
      "[TEXT]\n",
      "To solve this expression, I'll follow the order of operations (PEMDAS):\n",
      "\n",
      "First, calculate the expressions in parentheses:\n",
      "- (3 * 4) = 12\n",
      "- (5 * 6) = 30\n",
      "\n",
      "Then add all terms:\n",
      "1 + 2 + 12 + 30 = 45\n",
      "\n",
      "The answer is 45."
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    TextChunkEvent, ThinkingChunkEvent,\n",
    "    TextResponse, ReasoningEffort,\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    reasoning_effort=ReasoningEffort.MEDIUM,\n",
    ")\n",
    "messages=[user_message(\"What is 1 + 2 + (3 * 4) + (5 * 6)?\")]\n",
    "summary = None\n",
    "\n",
    "current_type = None\n",
    "async for response in client.stream(messages=messages):\n",
    "    is_text_chunk = isinstance(response, TextChunkEvent)\n",
    "    is_thinking_chunk = isinstance(response, ThinkingChunkEvent)\n",
    "    is_summary = isinstance(response, TextResponse)\n",
    "\n",
    "    if is_text_chunk or is_thinking_chunk:\n",
    "        if type(response) is not current_type:\n",
    "            print(f\"\\n\\n[{'THINKING' if is_thinking_chunk else 'TEXT'}]\")\n",
    "            current_type = type(response)\n",
    "        print(response.content, end=\"\")\n",
    "    elif isinstance(response, TextResponse):\n",
    "        summary = response\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response type: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextResponse(input_tokens=60, output_tokens=235, input_cost=0.00018, output_cost=0.003525, cache_write_tokens=0, cache_read_tokens=0, cache_write_cost=0.0, cache_read_cost=0.0, duration_seconds=3.8085758686065674, response=\"I need to calculate the value of the expression 1 + 2 + (3 * 4) + (5 * 6).\\n\\nFollowing the order of operations (PEMDAS), I need to do the multiplication first, then addition.\\n\\n(3 * 4) = 12\\n(5 * 6) = 30\\n\\nNow I have:\\n1 + 2 + 12 + 30\\n\\nAdding these up:\\n1 + 2 = 3\\n3 + 12 = 15\\n15 + 30 = 45\\n\\nSo the answer is 45.To solve this expression, I'll follow the order of operations (PEMDAS):\\n\\nFirst, calculate the expressions in parentheses:\\n- (3 * 4) = 12\\n- (5 * 6) = 30\\n\\nThen add all terms:\\n1 + 2 + 12 + 30 = 45\\n\\nThe answer is 45.\")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReasoningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUESTION]: I'm planning a trip to New York and Miami. What's the weather like in both cities? Also, if I have a budget of $2400 for a 6-day trip, how much can I spend per day?\n",
      "\n",
      "\n",
      "--- Iteration 1\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "To provide the weather information, I need to check the current weather for both New York and Miami. Additionally, I need to calculate the daily budget for a 6-day trip with a total budget of $2400. First, I will calculate the daily budget, and then I will check the weather for both cities.\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: get_weather`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"location\": \"New York\",\n",
      "  \"units\": \"F\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: get_weather`\n",
      "Result: {'New York': '68°F'}\n",
      "\n",
      "--- Iteration 2\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "I have obtained the current weather for New York, which is 68°F. Now, I will proceed to get the current weather for Miami. After that, I will calculate the daily budget for the trip.\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: get_weather`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"location\": \"Miami\",\n",
      "  \"units\": \"F\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: get_weather`\n",
      "Result: {'Miami': '85°F'}\n",
      "\n",
      "--- Iteration 3\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "I have obtained the current weather for both cities: New York is 68°F and Miami is 85°F. Now, I need to calculate the daily budget for the trip. With a total budget of $2400 for 6 days, I can find out how much can be spent per day by dividing the total budget by the number of days.\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: calculator`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"expression\": \"2400 / 6\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: calculator`\n",
      "Result: 400.0\n",
      "\n",
      "--- Iteration 4\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "I have calculated the daily budget, which is $400 per day for the trip. Now I have all the information needed: the weather in New York is 68°F, the weather in Miami is 85°F, and the daily budget is $400.\n",
      "\n",
      "[FINAL RESPONSE]:\n",
      "Final Answer:\n",
      "\n",
      "The current weather in New York is 68°F, while in Miami, it is 85°F. For your 6-day trip with a budget of $2400, you can spend $400 per day.\n",
      "\n",
      "Explanation:\n",
      "\n",
      "- The weather information was obtained for both cities, showing a significant difference in temperature, with Miami being warmer.\n",
      "- The daily budget was calculated by dividing the total budget of $2400 by the number of days (6), resulting in $400 available to spend each day. \n",
      "\n",
      "This information should help you plan your trip effectively!\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sik_llms.models_base import (\n",
    "    Tool, Parameter, ThinkingEvent, ToolPredictionEvent,\n",
    "    ToolResultEvent, TextChunkEvent, ErrorEvent, TextResponse,\n",
    ")\n",
    "from sik_llms.reasoning_agent import ReasoningAgent\n",
    "\n",
    "####\n",
    "# Define the tool functions\n",
    "####\n",
    "async def calculator(expression: str) -> str:\n",
    "    \"\"\"Execute calculator tool.\"\"\"\n",
    "    try:\n",
    "        # Only allow simple arithmetic for safety\n",
    "        allowed_chars = set('0123456789+-*/() .')\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return \"Error: Invalid characters in expression\"\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e!s}\"\n",
    "\n",
    "\n",
    "async def weather(location: str, units: str) -> str:\n",
    "    \"\"\"Mock weather tool - returns fake data.\"\"\"\n",
    "    # Return mock weather data\n",
    "    weather_data = {\n",
    "        'New York': '68',\n",
    "        'San Francisco': '62',\n",
    "        'Miami': '85',\n",
    "        'Chicago': '55',\n",
    "        'Los Angeles': '75',\n",
    "    }\n",
    "    for city in weather_data:  # noqa: PLC0206\n",
    "        if city.lower() in location.lower():\n",
    "            temp = weather_data[city]\n",
    "            if units == 'C':\n",
    "                # C = (°F - 32) x (5/9)\n",
    "                temp = round((temp - 32) * 5 / 9)\n",
    "            return {location: f\"{temp}°{units}\"}\n",
    "    return None\n",
    "\n",
    "####\n",
    "# Define tool objects\n",
    "####\n",
    "calculator_tool = Tool(\n",
    "    name='calculator',\n",
    "    description=\"Perform mathematical calculations\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='expression',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description=\"The mathematical expression to evaluate (e.g., '2 + 2', '5 * 10')\",\n",
    "        ),\n",
    "    ],\n",
    "    func=calculator,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather for a location\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name=\"location\",\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description=\"The name of the city (e.g., 'San Francisco', 'New York', 'London')\",\n",
    "        ),\n",
    "        Parameter(\n",
    "            name='units',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description=\"The units for temperature\",\n",
    "            valid_values=['F', 'C'],\n",
    "        ),\n",
    "    ],\n",
    "    func=weather,\n",
    ")\n",
    "\n",
    "# Create the reasoning agent\n",
    "agent = ReasoningAgent(\n",
    "    model_name=\"gpt-4o-mini\",  # You can change this to other models\n",
    "    tools=[calculator_tool, weather_tool],\n",
    "    max_iterations=10,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "question = \"I'm planning a trip to New York and Miami. What's the weather like in both cities? Also, if I have a budget of $2400 for a 6-day trip, how much can I spend per day?\"  # noqa: E501\n",
    "# Run the agent and collect the results\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "print(f\"[QUESTION]: {question}\\n\")\n",
    "\n",
    "current_iteration = 0\n",
    "\n",
    "async for event in agent.stream(messages):\n",
    "    if isinstance(event, ThinkingEvent):\n",
    "        if hasattr(event, 'iteration') and event.iteration != current_iteration:\n",
    "            current_iteration = event.iteration\n",
    "            print(f\"\\n--- Iteration {current_iteration}\\n\")\n",
    "        if event.content:\n",
    "            print(f\"\\n[THINKING]:\\n{event.content}\")\n",
    "\n",
    "    elif isinstance(event, ToolPredictionEvent):\n",
    "        print(\"\\n[TOOL PREDICTION]:\")\n",
    "        print(f\"Tool: {event.name}`\")\n",
    "        print(f\"Parameters: \\n```json\\n{json.dumps(event.arguments, indent=2)}\\n```\")\n",
    "\n",
    "    elif isinstance(event, ToolResultEvent):\n",
    "        print(\"\\n[TOOL RESULT]:\")\n",
    "        print(f\"Tool: {event.name}`\")\n",
    "        print(f\"Result: {event.result}\")\n",
    "\n",
    "    elif isinstance(event, ErrorEvent):\n",
    "        print(\"\\n[ERROR]:\")\n",
    "        print(f\"Error: {event.content}\")\n",
    "\n",
    "    elif isinstance(event, TextChunkEvent):\n",
    "        # For streaming text generation\n",
    "        if current_iteration  >= 0:  # Only print once for the header\n",
    "            print(\"\\n[FINAL RESPONSE]:\")\n",
    "            current_iteration = -1  # Set to an impossible iteration to avoid repeating\n",
    "\n",
    "        print(event.content, end=\"\")\n",
    "\n",
    "    elif isinstance(event, TextResponse):\n",
    "        # Print nothing here as we've already streamed the response\n",
    "        pass\n",
    "\n",
    "# Just for notebook display cleanliness\n",
    "print(\"\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens: 5013\n",
      "Output Tokens: 503\n",
      "Total Cost: 0.00105375\n",
      "Duration: 10.58 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input Tokens: {event.input_tokens}\")\n",
    "print(f\"Output Tokens: {event.output_tokens}\")\n",
    "print(f\"Total Cost: {event.total_cost}\")\n",
    "print(f\"Duration: {event.duration_seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic - `cache_control` parameter in `system_message`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mean great page four learn. Mr type others gas perhaps. Near least leg say.\\nCould plan station foot material bring. Individual live answer scene. Catch federal theory exist for citizen on recent. Resp'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faker import Faker\n",
    "\n",
    "cache_content = Faker().text(max_nb_chars=15_000)\n",
    "cache_content[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first word of the cached text is \"Mean\".\n",
      "---\n",
      "Total Cost: 0.0029235999999999997\n",
      "---\n",
      "Input Tokens: 17\n",
      "Output Tokens: 14\n",
      "Cache Write Tokens: 2854\n",
      "Cache Read Tokens: 0\n",
      "Total Tokens: 2885\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This example is modified from anthropic's prompt-caching.ipynb notebook\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "####\n",
    "from sik_llms import Anthropic\n",
    "from sik_llms.models_base import system_message, user_message, assistant_message\n",
    "\n",
    "client = Anthropic(\n",
    "    model_name='claude-3-5-haiku-latest',\n",
    "    temperature=0.1,\n",
    ")\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "system_messages = [\n",
    "    system_message(\"You are a helpful assistant.\"),\n",
    "    system_message(\n",
    "        cache_content,\n",
    "        cache_control={'type': 'ephemeral'},\n",
    "    ),\n",
    "]\n",
    "messages = [\n",
    "    *system_messages,\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "]\n",
    "\n",
    "# first run should result in a cache-miss & write\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second word of the cached text is \"great\".\n",
      "---\n",
      "Total Cost: 0.00031952\n",
      "---\n",
      "Input Tokens: 44\n",
      "Output Tokens: 14\n",
      "Cache Write Tokens: 0\n",
      "Cache Read Tokens: 2854\n",
      "Total Tokens: 2912\n"
     ]
    }
   ],
   "source": [
    "# second run should result in a cache-hit & read\n",
    "messages = [\n",
    "    *system_messages,\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "    assistant_message(response.response),\n",
    "    user_message(\"What is the second word of the cached text?\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic - `cache_content` parameter in `__init__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Home attention behavior agent plan. Writer stock direction beyond entire cup free.\\nShe mother floor fact reason hope culture. Way put card life someone best example interest. Traditional behavior over'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faker import Faker\n",
    "\n",
    "cache_content = Faker().text(max_nb_chars=15_000)\n",
    "cache_content[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first word of the cached text is \"Home\".\n",
      "---\n",
      "Total Cost: 0.0029235999999999997\n",
      "---\n",
      "Input Tokens: 17\n",
      "Output Tokens: 14\n",
      "Cache Write Tokens: 2854\n",
      "Cache Read Tokens: 0\n",
      "Total Tokens: 2885\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This example is modified from anthropic's prompt-caching.ipynb notebook\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "####\n",
    "from sik_llms import Anthropic\n",
    "from sik_llms.models_base import system_message, user_message, assistant_message\n",
    "\n",
    "client = Anthropic(\n",
    "    model_name='claude-3-5-haiku-latest',\n",
    "    temperature=0.1,\n",
    "    cache_content=cache_content,\n",
    ")\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "messages = [\n",
    "    system_message(\"You are a helpful assistant.\"),\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "]\n",
    "\n",
    "# first run should result in a cache-miss & write\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second word of the cached text is \"attention\".\n",
      "---\n",
      "Total Cost: 0.00031952\n",
      "---\n",
      "Input Tokens: 44\n",
      "Output Tokens: 14\n",
      "Cache Write Tokens: 0\n",
      "Cache Read Tokens: 2854\n",
      "Total Tokens: 2912\n"
     ]
    }
   ],
   "source": [
    "# second run should result in a cache-hit & read\n",
    "messages = [\n",
    "    system_message(\"You are a helpful assistant.\"),\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "    assistant_message(response.response),\n",
    "    user_message(\"What is the second word of the cached text?\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock via OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sik_llms import OpenAI, user_message\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    server_url=os.getenv('BEDROCK_API_URL'),\n",
    "    api_key=os.getenv('BEDROCK_API_KEY'),\n",
    "    model_name='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "    user='bedrock-requires-user?',\n",
    ")\n",
    "response = client(messages=[user_message(\"What is the capital of France?\")])\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Log Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End (logprob: -1.61, prob: 0.20)\n",
      "less (logprob: -0.00, prob: 1.00)\n",
      " waves (logprob: -0.03, prob: 0.97)\n",
      " whisper (logprob: -0.12, prob: 0.88)\n",
      ", (logprob: -0.00, prob: 1.00)\n",
      "  \n",
      " (logprob: -0.00, prob: 1.00)\n",
      "Secrets (logprob: -0.86, prob: 0.43)\n",
      " of (logprob: -0.06, prob: 0.94)\n",
      " the (logprob: -0.02, prob: 0.98)\n",
      " deep (logprob: -0.00, prob: 1.00)\n",
      " blue (logprob: -1.06, prob: 0.35)\n",
      " world (logprob: -1.08, prob: 0.34)\n",
      ", (logprob: -0.04, prob: 0.96)\n",
      "  \n",
      " (logprob: -0.00, prob: 1.00)\n",
      "Salt (logprob: -1.22, prob: 0.30)\n",
      " and (logprob: -1.60, prob: 0.20)\n",
      " sun (logprob: -0.29, prob: 0.75)\n",
      " embrace (logprob: -0.70, prob: 0.50)\n",
      ". (logprob: -0.00, prob: 1.00)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sik_llms import OpenAI, user_message, TextChunkEvent\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    model_name='gpt-4o-mini',\n",
    "    logprobs=True,\n",
    ")\n",
    "messages = [\n",
    "    user_message(\"Write a haiku about the ocean.\"),\n",
    "]\n",
    "async for response in client.stream(messages=messages):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        log_prob = response.logprob\n",
    "        prob = math.exp(log_prob)\n",
    "        print(f\"{response.content} (logprob: {log_prob:.2f}, prob: {prob:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=8516 output_tokens=53 input_cost=0.0012774 output_cost=3.18e-05 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=1.5114619731903076 response='The image features a wooden boardwalk or pathway leading through a grassy landscape. The path extends into a wide-open area covered with tall green grass, surrounded by trees and bushes in the background. The sky above is bright with fluffy clouds, suggesting a pleasant day.'\n",
      "---\n",
      "The image features a wooden boardwalk or pathway leading through a grassy landscape. The path extends into a wide-open area covered with tall green grass, surrounded by trees and bushes in the background. The sky above is bright with fluffy clouds, suggesting a pleasant day.\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import create_client, user_message, ImageContent\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "image = ImageContent.from_url(\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/320px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    ")\n",
    "\n",
    "client = create_client(model_name='gpt-4o-mini')\n",
    "response = client(messages=[\n",
    "    user_message([\n",
    "        \"What's in this image? Describe it briefly.\",\n",
    "        image,\n",
    "    ]),\n",
    "])\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
