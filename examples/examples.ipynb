{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanekercheval/repos/sik-llms/src\n"
     ]
    }
   ],
   "source": [
    "%cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.openai.OpenAI at 0x10ce9e660>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For \"registered\" clients (via `@Client.register`), the client\n",
    "# can be created with `create_client` by passing in the model name.\n",
    "from sik_llms import create_client\n",
    "\n",
    "client = create_client(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.openai.OpenAI at 0x10d597610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, the client can be directly instantiated\n",
    "from sik_llms import OpenAI\n",
    "client = OpenAI(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.anthropic.Anthropic at 0x10d6d92b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, the client can be directly instantiated\n",
    "from sik_llms import Anthropic\n",
    "client = Anthropic(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user', 'content': 'What is the capital of France?'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sik_llms import create_client, user_message, ResponseChunk\n",
    "\n",
    "client = create_client(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the capital of France?\")\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Synchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResponseSummary(input_tokens=14, output_tokens=7, input_cost=2.1e-06, output_cost=4.2e-06, duration_seconds=0.5157272815704346, content='The capital of France is Paris.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client(messages=[message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "async for response in client.run_async(messages=[message]):\n",
    "    if isinstance(response, ResponseChunk):\n",
    "        print(response.content, end=\"\")\n",
    "        responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=14 output_tokens=7 input_cost=2.1e-06 output_cost=4.2e-06 duration_seconds=0.6948091983795166 content='The capital of France is Paris.'\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseSummary(input_tokens=14, output_tokens=50, input_cost=4.2000000000000004e-05, output_cost=0.00075, duration_seconds=1.0311648845672607, content=\"The capital of France is Paris. It's not only the capital city but also the largest city in France, known for its iconic landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = create_client(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    temperature=0.1,\n",
    ")\n",
    "response = client(messages=[user_message(\"What is the capital of France?\")])\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Functions/Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=60 output_tokens=17 input_cost=9e-06 output_cost=1.0199999999999999e-05 duration_seconds=0.9957671165466309 function_call=FunctionCallResult(name='get_weather', arguments={'location': 'Paris, France'}, call_id='call_W40jeAvXlnCRloLc5AS2wner') message=None\n",
      "---\n",
      "name='get_weather' arguments={'location': 'Paris, France'} call_id='call_W40jeAvXlnCRloLc5AS2wner'\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import Tool, Parameter, RegisteredClients\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the weather for a location.\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name=\"location\",\n",
    "            type=\"string\",\n",
    "            required=True,\n",
    "            description=\"The city and country for weather info.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    client_type=RegisteredClients.OPENAI_TOOLS,\n",
    "    model_name='gpt-4o-mini',\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the weather in Paris?\")\n",
    "response = await client.run_async(messages=[message])\n",
    "# or `response = client(messages=[message])` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.tool_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude Functions/Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=394 output_tokens=40 input_cost=0.001182 output_cost=0.0006000000000000001 duration_seconds=1.541290044784546 function_call=FunctionCallResult(name='get_weather', arguments={'location': 'Paris, France'}, call_id='toolu_01Nkx1rciEp731kdKTYDAfHE') message=None\n",
      "---\n",
      "name='get_weather' arguments={'location': 'Paris, France'} call_id='toolu_01Nkx1rciEp731kdKTYDAfHE'\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client,\n",
    "    user_message,\n",
    "    Tool,\n",
    "    Parameter,\n",
    "    RegisteredClients,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the weather for a location.\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name=\"location\",\n",
    "            type=\"string\",\n",
    "            required=True,\n",
    "            description=\"The city and country for weather info.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    client_type=RegisteredClients.ANTHROPIC_TOOLS,\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the weather in Paris?\")\n",
    "response = await client.run_async(messages=[message])\n",
    "# or `response = client(messages=[message])` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.tool_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs via OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=92 output_tokens=18 input_cost=1.38e-05 output_cost=1.08e-05 duration_seconds=1.2821071147918701 content=StructuredOutputResponse(parsed=CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob']), refusal=None)\n",
      "---\n",
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):  # noqa: D101\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "client = create_client(\n",
    "    model_name='gpt-4o-mini',\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "messages=[\n",
    "    system_message(\"Extract the event information.\"),\n",
    "    user_message(\"Alice and Bob are going to a science fair on Friday.\"),\n",
    "]\n",
    "response = client(messages=messages)\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.content.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs via Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=424 output_tokens=78 input_cost=0.0012720000000000001 output_cost=0.00117 duration_seconds=1.8466229438781738 content=StructuredOutputResponse(parsed=CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob']), refusal=None)\n",
      "---\n",
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):  # noqa: D101\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "client = create_client(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "messages=[\n",
    "    system_message(\"Extract the event information.\"),\n",
    "    user_message(\"Alice and Bob are going to a science fair on Friday.\"),\n",
    "]\n",
    "response = client(messages=messages)\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.content.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning via OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down step by step:\n",
      "\n",
      "1. First, multiply 3 by 4: 3 * 4 = 12.\n",
      "2. Next, multiply 5 by 6: 5 * 6 = 30.\n",
      "3. Now add up all the numbers: 1 + 2 + 12 + 30 = 45.\n",
      "\n",
      "So, the answer is 45."
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client,\n",
    "    user_message,\n",
    "    ResponseChunk,\n",
    "    ResponseSummary,\n",
    "    ReasoningEffort,\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    model_name='o3-mini',\n",
    "    reasoning_effort=ReasoningEffort.MEDIUM,\n",
    ")\n",
    "messages=[user_message(\"What is 1 + 2 + (3 * 4) + (5 * 6)?\")]\n",
    "summary = None\n",
    "async for response in client.run_async(messages=messages):\n",
    "    if isinstance(response, ResponseChunk):\n",
    "        print(response.content, end=\"\")\n",
    "    elif isinstance(response, ResponseSummary):\n",
    "        summary = response\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response type: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseSummary(input_tokens=28, output_tokens=80, input_cost=3.08e-05, output_cost=0.000352, duration_seconds=3.944333076477051, content=\"Let's break it down step by step:\\n\\n1. First, multiply 3 by 4: 3 * 4 = 12.\\n2. Next, multiply 5 by 6: 5 * 6 = 30.\\n3. Now add up all the numbers: 1 + 2 + 12 + 30 = 45.\\n\\nSo, the answer is 45.\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning via Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[THINKING]\n",
      "I need to solve the given arithmetic expression, following the order of operations (PEMDAS):\n",
      "\n",
      "1 + 2 + (3 * 4) + (5 * 6)\n",
      "\n",
      "First, I'll evaluate the expressions in parentheses:\n",
      "(3 * 4) = 12\n",
      "(5 * 6) = 30\n",
      "\n",
      "Now I have:\n",
      "1 + 2 + 12 + 30\n",
      "\n",
      "Adding all the terms:\n",
      "1 + 2 = 3\n",
      "3 + 12 = 15\n",
      "15 + 30 = 45\n",
      "\n",
      "So the final result is 45.\n",
      "\n",
      "[TEXT]\n",
      "To solve this expression, I'll follow the order of operations (PEMDAS):\n",
      "\n",
      "1 + 2 + (3 * 4) + (5 * 6)\n",
      "\n",
      "First, I'll evaluate the expressions in parentheses:\n",
      "(3 * 4) = 12\n",
      "(5 * 6) = 30\n",
      "\n",
      "Now the expression becomes:\n",
      "1 + 2 + 12 + 30\n",
      "\n",
      "Adding all these terms:\n",
      "1 + 2 + 12 + 30 = 45\n",
      "\n",
      "The answer is 45."
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client,\n",
    "    user_message,\n",
    "    ResponseChunk,\n",
    "    ResponseSummary,\n",
    "    ReasoningEffort,\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    reasoning_effort=ReasoningEffort.MEDIUM,\n",
    ")\n",
    "messages=[user_message(\"What is 1 + 2 + (3 * 4) + (5 * 6)?\")]\n",
    "summary = None\n",
    "\n",
    "current_type = None\n",
    "async for response in client.run_async(messages=messages):\n",
    "    if isinstance(response, ResponseChunk):\n",
    "        if response.content_type != current_type:\n",
    "            print(f\"\\n\\n[{response.content_type.name}]\")\n",
    "            current_type = response.content_type\n",
    "        print(response.content, end=\"\")\n",
    "    elif isinstance(response, ResponseSummary):\n",
    "        summary = response\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response type: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
