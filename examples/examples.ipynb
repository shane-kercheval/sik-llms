{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanekercheval/repos/sik-llms/src\n"
     ]
    }
   ],
   "source": [
    "%cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = 'gpt-5-mini'\n",
    "CLAUDE_MODEL = 'claude-sonnet-4-5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.openai.OpenAI at 0x1054ac8d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For \"registered\" clients (via `@Client.register`), the client\n",
    "# can be created with `create_client` by passing in the model name.\n",
    "from sik_llms import create_client\n",
    "\n",
    "client = create_client(\n",
    "    model_name=OPENAI_MODEL,\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.openai.OpenAI at 0x110e8df90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, the client can be directly instantiated\n",
    "from sik_llms import OpenAI\n",
    "client = OpenAI(\n",
    "    model_name=OPENAI_MODEL,\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.anthropic.Anthropic at 0x110e8fb50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, the client can be directly instantiated\n",
    "from sik_llms import Anthropic\n",
    "client = Anthropic(\n",
    "    model_name=CLAUDE_MODEL,\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user', 'content': 'What is the capital of France?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sik_llms import create_client, user_message, TextChunkEvent\n",
    "\n",
    "client = create_client(\n",
    "    model_name=OPENAI_MODEL,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the capital of France?\")\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Synchronously via `__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=13 output_tokens=16 input_cost=3.25e-06 output_cost=3.2e-05 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=1.6645826250314713 response='The capital of France is Paris.'\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = client(messages=[message])\n",
    "print(response)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Aynchronously via `run_async`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=13 output_tokens=16 input_cost=3.25e-06 output_cost=3.2e-05 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=1.5292387079680339 response='The capital of France is Paris.'\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = await client.run_async(messages=[message])\n",
    "print(response)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Asynchronously via `stream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "async for response in client.stream(messages=[message]):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        print(response.content, end=\"\")\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextChunkEvent(content='The', logprob=None),\n",
       " TextChunkEvent(content=' capital', logprob=None),\n",
       " TextChunkEvent(content=' of', logprob=None),\n",
       " TextChunkEvent(content=' France', logprob=None),\n",
       " TextChunkEvent(content=' is', logprob=None),\n",
       " TextChunkEvent(content=' Paris', logprob=None),\n",
       " TextChunkEvent(content='.', logprob=None),\n",
       " TextResponse(input_tokens=13, output_tokens=80, input_cost=3.25e-06, output_cost=0.00015999999999999999, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=1.6412498749559745, response='The capital of France is Paris.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate multiple responses from a single input via `sample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['45', '53724', '42']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TextResponse(input_tokens=16, output_tokens=1, input_cost=2.4e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.4291734170401469, response='45'),\n",
       " TextResponse(input_tokens=16, output_tokens=2, input_cost=2.4e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.4908954999409616, response='53724'),\n",
       " TextResponse(input_tokens=16, output_tokens=1, input_cost=2.4e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.4522558340104297, response='42')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = create_client(model_name='gpt-4o-mini', temperature=1.5)\n",
    "responses = await client.sample(\n",
    "    messages=[user_message('Generate a random number. Return only the number')],\n",
    "    n=3,\n",
    ")\n",
    "print([r.response for r in responses])\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate multiple responses from multiple inputs via `generate_multiple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paris', 'Rome']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TextResponse(input_tokens=19, output_tokens=10, input_cost=4.749999999999999e-06, output_cost=1.9999999999999998e-05, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=1.358596707927063, response='Paris'),\n",
       " TextResponse(input_tokens=19, output_tokens=74, input_cost=4.749999999999999e-06, output_cost=0.000148, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=1.9585146250901744, response='Rome')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = create_client(model_name=OPENAI_MODEL)\n",
    "responses = await client.generate_multiple(\n",
    "    messages=[\n",
    "        [user_message(\"What is the capital of France? Return only the city name.\")],\n",
    "        [user_message(\"What is the capital of Italy? Return only the city name.\")],\n",
    "    ],\n",
    ")\n",
    "print([r.response for r in responses])\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate `sample_n` responses from multiple inputs via `generate_multiple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['37', '42', '57', '47', '72']\n",
      "['157', '157', '147', '147', '153']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.4226528749568388, response='37'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.40700400003697723, response='42'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.390481291920878, response='57'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.38550245901569724, response='47'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.7454377920366824, response='72')],\n",
       " [TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.7168584589380771, response='157'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.45303025003522635, response='157'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.4449627499561757, response='147'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.3888425000477582, response='147'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.5163324580062181, response='153')]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = create_client(model_name='gpt-4o-mini', temperature=1.5)\n",
    "responses_set = await client.generate_multiple(\n",
    "    messages=[\n",
    "        [user_message(\"Pick a random number between 1 and 100. Return only the number.\")],\n",
    "        [user_message(\"Pick a random number between 100 and 200. Return only the number.\")],\n",
    "    ],\n",
    "    sample_n=5,\n",
    ")\n",
    "for responses in responses_set:\n",
    "    print([r.response for r in responses])\n",
    "responses_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Functions/Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=141 output_tokens=89 input_cost=3.5249999999999996e-05 output_cost=0.000178 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=1.85469154198654 tool_prediction=ToolPrediction(name='get_weather', arguments={'location': 'Paris, France'}, call_id='call_xvtWMQSARmodw5bee1elKPfv') message=None\n",
      "---\n",
      "name='get_weather' arguments={'location': 'Paris, France'} call_id='call_xvtWMQSARmodw5bee1elKPfv'\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    Tool, Parameter, RegisteredClients,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name='get_weather',\n",
    "    description=\"Get the weather for a location.\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='location',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description='The city and country for weather info.',\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    client_type=RegisteredClients.OPENAI_TOOLS,\n",
    "    model_name=OPENAI_MODEL,\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the weather in Paris?\")\n",
    "response = await client.run_async(messages=[message])\n",
    "# or `response = client(messages=[message])` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.tool_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude Functions/Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=402 output_tokens=40 input_cost=0.001206 output_cost=0.0006000000000000001 cache_write_tokens=0 cache_read_tokens=0 cache_write_cost=0.0 cache_read_cost=0.0 duration_seconds=1.1365794580196962 tool_prediction=ToolPrediction(name='get_weather', arguments={'location': 'Paris, France'}, call_id='toolu_01SZbbKwFk2JeCgiRhwwrj6m') message=None\n",
      "---\n",
      "name='get_weather' arguments={'location': 'Paris, France'} call_id='toolu_01SZbbKwFk2JeCgiRhwwrj6m'\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    Tool, Parameter, RegisteredClients,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name='get_weather',\n",
    "    description=\"Get the weather for a location.\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='location',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description='The city and country for weather info.',\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    client_type=RegisteredClients.ANTHROPIC_TOOLS,\n",
    "    model_name=CLAUDE_MODEL,\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the weather in Paris?\")\n",
    "response = await client.run_async(messages=[message])\n",
    "# or `response = client(messages=[message])` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.tool_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs via OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=90 output_tokens=222 input_cost=2.2499999999999998e-05 output_cost=0.000444 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=4.257336625014432 parsed=CalendarEvent(name='Science fair', date='Friday', participants=['Alice', 'Bob']) refusal=None\n",
      "---\n",
      "name='Science fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):  # noqa: D101\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "client = create_client(\n",
    "    model_name=OPENAI_MODEL,\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "messages=[\n",
    "    system_message(\"Extract the event information.\"),\n",
    "    user_message(\"Alice and Bob are going to a science fair on Friday.\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "# or `response = client(messages=messages)` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs via Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=442 output_tokens=78 input_cost=0.0013260000000000001 output_cost=0.00117 cache_write_tokens=None cache_read_tokens=None cache_write_cost=None cache_read_cost=None duration_seconds=1.6880054170032963 parsed=CalendarEvent(name='Science fair', date='Friday', participants=['Alice', 'Bob']) refusal=None\n",
      "---\n",
      "name='Science fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):  # noqa: D101\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "client = create_client(\n",
    "    model_name=CLAUDE_MODEL,\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "messages=[\n",
    "    system_message(\"Extract the event information.\"),\n",
    "    user_message(\"Alice and Bob are going to a science fair on Friday.\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "# or `response = client(messages=messages)` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning via OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down step by step:\n",
      "\n",
      "1. Evaluate the multiplications:\n",
      "   - 3 * 4 = 12\n",
      "   - 5 * 6 = 30\n",
      "\n",
      "2. Now substitute back into the expression:\n",
      "   1 + 2 + 12 + 30\n",
      "\n",
      "3. Then add everything together:\n",
      "   1 + 2 = 3  \n",
      "   3 + 12 = 15  \n",
      "   15 + 30 = 45\n",
      "\n",
      "So, the answer is 45."
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    TextChunkEvent, TextResponse, ReasoningEffort,\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    model_name='o3-mini',\n",
    "    reasoning_effort=ReasoningEffort.MEDIUM,\n",
    ")\n",
    "messages=[user_message(\"What is 1 + 2 + (3 * 4) + (5 * 6)?\")]\n",
    "summary = None\n",
    "async for response in client.stream(messages=messages):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        print(response.content, end=\"\")\n",
    "    elif isinstance(response, TextResponse):\n",
    "        summary = response\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response type: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextResponse(input_tokens=27, output_tokens=246, input_cost=2.97e-05, output_cost=0.0010824, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=2.1931866669328883, response=\"Let's break it down step by step:\\n\\n1. Evaluate the multiplications:\\n   - 3 * 4 = 12\\n   - 5 * 6 = 30\\n\\n2. Now substitute back into the expression:\\n   1 + 2 + 12 + 30\\n\\n3. Then add everything together:\\n   1 + 2 = 3  \\n   3 + 12 = 15  \\n   15 + 30 = 45\\n\\nSo, the answer is 45.\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning via Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[THINKING]\n",
      "I need to solve this step by step, following the order of operations (PEMDAS/BODMAS).\n",
      "\n",
      "The expression is: 1 + 2 + (3 * 4) + (5 * 6)\n",
      "\n",
      "First, I'll handle the operations in parentheses:\n",
      "- (3 * 4) = 12\n",
      "- (5 * 6) = 30\n",
      "\n",
      "Now the expression becomes: 1 + 2 + 12 + 30\n",
      "\n",
      "Now I can add from left to right:\n",
      "- 1 + 2 = 3\n",
      "- 3 + 12 = 15\n",
      "- 15 + 30 = 45\n",
      "\n",
      "[TEXT]\n",
      "I'll solve this step by step, following the order of operations.\n",
      "\n",
      "1 + 2 + (3 * 4) + (5 * 6)\n",
      "\n",
      "First, I'll calculate the multiplication in parentheses:\n",
      "- (3 * 4) = 12\n",
      "- (5 * 6) = 30\n",
      "\n",
      "Now the expression becomes:\n",
      "1 + 2 + 12 + 30\n",
      "\n",
      "Adding from left to right:\n",
      "1 + 2 + 12 + 30 = 45\n",
      "\n",
      "The answer is **45**."
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    TextChunkEvent, ThinkingChunkEvent,\n",
    "    TextResponse, ReasoningEffort,\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    model_name=CLAUDE_MODEL,\n",
    "    reasoning_effort=ReasoningEffort.MEDIUM,\n",
    ")\n",
    "messages=[user_message(\"What is 1 + 2 + (3 * 4) + (5 * 6)?\")]\n",
    "summary = None\n",
    "\n",
    "current_type = None\n",
    "async for response in client.stream(messages=messages):\n",
    "    is_text_chunk = isinstance(response, TextChunkEvent)\n",
    "    is_thinking_chunk = isinstance(response, ThinkingChunkEvent)\n",
    "    is_summary = isinstance(response, TextResponse)\n",
    "\n",
    "    if is_text_chunk or is_thinking_chunk:\n",
    "        if type(response) is not current_type:\n",
    "            print(f\"\\n\\n[{'THINKING' if is_thinking_chunk else 'TEXT'}]\")\n",
    "            current_type = type(response)\n",
    "        print(response.content, end=\"\")\n",
    "    elif isinstance(response, TextResponse):\n",
    "        summary = response\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response type: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextResponse(input_tokens=60, output_tokens=293, input_cost=0.00018, output_cost=0.0043950000000000005, cache_write_tokens=0, cache_read_tokens=0, cache_write_cost=0.0, cache_read_cost=0.0, duration_seconds=3.489773334003985, response=\"I need to solve this step by step, following the order of operations (PEMDAS/BODMAS).\\n\\nThe expression is: 1 + 2 + (3 * 4) + (5 * 6)\\n\\nFirst, I'll handle the operations in parentheses:\\n- (3 * 4) = 12\\n- (5 * 6) = 30\\n\\nNow the expression becomes: 1 + 2 + 12 + 30\\n\\nNow I can add from left to right:\\n- 1 + 2 = 3\\n- 3 + 12 = 15\\n- 15 + 30 = 45I'll solve this step by step, following the order of operations.\\n\\n1 + 2 + (3 * 4) + (5 * 6)\\n\\nFirst, I'll calculate the multiplication in parentheses:\\n- (3 * 4) = 12\\n- (5 * 6) = 30\\n\\nNow the expression becomes:\\n1 + 2 + 12 + 30\\n\\nAdding from left to right:\\n1 + 2 + 12 + 30 = 45\\n\\nThe answer is **45**.\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReasoningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUESTION]: I'm planning a trip to New York and Miami. What's the weather like in both cities? Also, if I have a budget of $2400 for a 6-day trip, how much can I spend per day?\n",
      "\n",
      "\n",
      "--- Iteration 1\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "I need the current weather for New York and Miami. I'll first fetch the weather for New York using the weather tool.\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: get_weather`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"location\": \"New York\",\n",
      "  \"units\": \"F\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: get_weather`\n",
      "Result: {'New York': '68°F'}\n",
      "\n",
      "--- Iteration 2\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "The get_weather result shows New York is 68°F — a mild, comfortable temperature. Next I should fetch the current weather for Miami so I can report both cities. After that I'll compute the daily budget from $2400 for 6 days ($2400 ÷ 6).\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: get_weather`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"location\": \"Miami\",\n",
      "  \"units\": \"F\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: get_weather`\n",
      "Result: {'Miami': '85°F'}\n",
      "\n",
      "--- Iteration 3\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "The get_weather results are New York: 68°F and Miami: 85°F — NY is mild/comfortable, Miami is hot. Now compute the daily budget: $2400 ÷ 6 days = $400 per day. I have enough information to provide the final answer.\n",
      "\n",
      "[FINAL RESPONSE]:\n",
      "Current conditions (from the checks I ran):\n",
      "- New York: 68°F — mild and comfortable; a light jacket or long-sleeve should be fine.\n",
      "- Miami: 85°F — warm/hot; lightweight clothing, sunscreen, and hydration recommended.\n",
      "\n",
      "Budget:\n",
      "- Total budget $2,400 ÷ 6 days = $400 per day.\n",
      "\n",
      "If you want, I can break that $400/day down into categories (lodging, food, transport, activities) or look up hourly forecasts/forecasts for specific dates.\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sik_llms.models_base import (\n",
    "    Tool, Parameter, ThinkingEvent, ToolPredictionEvent,\n",
    "    ToolResultEvent, TextChunkEvent, ErrorEvent, TextResponse,\n",
    ")\n",
    "from sik_llms.reasoning_agent import ReasoningAgent\n",
    "\n",
    "####\n",
    "# Define the tool functions\n",
    "####\n",
    "async def calculator(expression: str) -> str:\n",
    "    \"\"\"Execute calculator tool.\"\"\"\n",
    "    try:\n",
    "        # Only allow simple arithmetic for safety\n",
    "        allowed_chars = set('0123456789+-*/() .')\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return \"Error: Invalid characters in expression\"\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e!s}\"\n",
    "\n",
    "\n",
    "async def weather(location: str, units: str) -> str:\n",
    "    \"\"\"Mock weather tool - returns fake data.\"\"\"\n",
    "    # Return mock weather data\n",
    "    weather_data = {\n",
    "        'New York': '68',\n",
    "        'San Francisco': '62',\n",
    "        'Miami': '85',\n",
    "        'Chicago': '55',\n",
    "        'Los Angeles': '75',\n",
    "    }\n",
    "    for city in weather_data:  # noqa: PLC0206\n",
    "        if city.lower() in location.lower():\n",
    "            temp = weather_data[city]\n",
    "            if units == 'C':\n",
    "                # C = (°F - 32) x (5/9)\n",
    "                temp = round((temp - 32) * 5 / 9)\n",
    "            return {location: f\"{temp}°{units}\"}\n",
    "    return None\n",
    "\n",
    "####\n",
    "# Define tool objects\n",
    "####\n",
    "calculator_tool = Tool(\n",
    "    name='calculator',\n",
    "    description=\"Perform mathematical calculations\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='expression',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description=\"The mathematical expression to evaluate (e.g., '2 + 2', '5 * 10')\",\n",
    "        ),\n",
    "    ],\n",
    "    func=calculator,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather for a location\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name=\"location\",\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description=\"The name of the city (e.g., 'San Francisco', 'New York', 'London')\",\n",
    "        ),\n",
    "        Parameter(\n",
    "            name='units',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description=\"The units for temperature\",\n",
    "            valid_values=['F', 'C'],\n",
    "        ),\n",
    "    ],\n",
    "    func=weather,\n",
    ")\n",
    "\n",
    "# Create the reasoning agent\n",
    "agent = ReasoningAgent(\n",
    "    model_name=OPENAI_MODEL,  # You can change this to other models\n",
    "    tools=[calculator_tool, weather_tool],\n",
    "    max_iterations=10,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "question = \"I'm planning a trip to New York and Miami. What's the weather like in both cities? Also, if I have a budget of $2400 for a 6-day trip, how much can I spend per day?\"  # noqa: E501\n",
    "# Run the agent and collect the results\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "print(f\"[QUESTION]: {question}\\n\")\n",
    "\n",
    "current_iteration = 0\n",
    "\n",
    "async for event in agent.stream(messages):\n",
    "    if isinstance(event, ThinkingEvent):\n",
    "        if hasattr(event, 'iteration') and event.iteration != current_iteration:\n",
    "            current_iteration = event.iteration\n",
    "            print(f\"\\n--- Iteration {current_iteration}\\n\")\n",
    "        if event.content:\n",
    "            print(f\"\\n[THINKING]:\\n{event.content}\")\n",
    "\n",
    "    elif isinstance(event, ToolPredictionEvent):\n",
    "        print(\"\\n[TOOL PREDICTION]:\")\n",
    "        print(f\"Tool: {event.name}`\")\n",
    "        print(f\"Parameters: \\n```json\\n{json.dumps(event.arguments, indent=2)}\\n```\")\n",
    "\n",
    "    elif isinstance(event, ToolResultEvent):\n",
    "        print(\"\\n[TOOL RESULT]:\")\n",
    "        print(f\"Tool: {event.name}`\")\n",
    "        print(f\"Result: {event.result}\")\n",
    "\n",
    "    elif isinstance(event, ErrorEvent):\n",
    "        print(\"\\n[ERROR]:\")\n",
    "        print(f\"Error: {event.content}\")\n",
    "\n",
    "    elif isinstance(event, TextChunkEvent):\n",
    "        # For streaming text generation\n",
    "        if current_iteration  >= 0:  # Only print once for the header\n",
    "            print(\"\\n[FINAL RESPONSE]:\")\n",
    "            current_iteration = -1  # Set to an impossible iteration to avoid repeating\n",
    "\n",
    "        print(event.content, end=\"\")\n",
    "\n",
    "    elif isinstance(event, TextResponse):\n",
    "        # Print nothing here as we've already streamed the response\n",
    "        pass\n",
    "\n",
    "# Just for notebook display cleanliness\n",
    "print(\"\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens: 3402\n",
      "Output Tokens: 2736\n",
      "Total Cost: 0.006322499999999999\n",
      "Duration: 43.08 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input Tokens: {event.input_tokens}\")\n",
    "print(f\"Output Tokens: {event.output_tokens}\")\n",
    "print(f\"Total Cost: {event.total_cost}\")\n",
    "print(f\"Duration: {event.duration_seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic - `cache_control` parameter in `system_message`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Begin whom area in along clear scientist. Tough effort open civil fish side.\\nWhether road ask be employee. Hot direction work game where a color. Energy strong go someone strong.\\nFeel may national. Sp'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faker import Faker\n",
    "\n",
    "cache_content = Faker().text(max_nb_chars=15_000)\n",
    "cache_content[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first word of the cached text is \"Begin\".\n",
      "---\n",
      "Total Cost: 0.011061000000000001\n",
      "---\n",
      "Input Tokens: 17\n",
      "Output Tokens: 14\n",
      "Cache Write Tokens: 2880\n",
      "Cache Read Tokens: 0\n",
      "Total Tokens: 2911\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This example is modified from anthropic's prompt-caching.ipynb notebook\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "####\n",
    "from sik_llms import Anthropic\n",
    "from sik_llms.models_base import system_message, user_message, assistant_message\n",
    "\n",
    "client = Anthropic(\n",
    "    model_name=CLAUDE_MODEL,\n",
    "    temperature=0.1,\n",
    ")\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "system_messages = [\n",
    "    system_message(\"You are a helpful assistant.\"),\n",
    "    system_message(\n",
    "        cache_content,\n",
    "        cache_control={'type': 'ephemeral'},\n",
    "    ),\n",
    "]\n",
    "messages = [\n",
    "    *system_messages,\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "]\n",
    "\n",
    "# first run should result in a cache-miss & write\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second word of the cached text is \"whom\".\n",
      "---\n",
      "Total Cost: 0.001206\n",
      "---\n",
      "Input Tokens: 44\n",
      "Output Tokens: 14\n",
      "Cache Write Tokens: 0\n",
      "Cache Read Tokens: 2880\n",
      "Total Tokens: 2938\n"
     ]
    }
   ],
   "source": [
    "# second run should result in a cache-hit & read\n",
    "messages = [\n",
    "    *system_messages,\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "    assistant_message(response.response),\n",
    "    user_message(\"What is the second word of the cached text?\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic - `cache_content` parameter in `__init__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whose send space him part. Imagine back offer could threat chair western picture. Close reach prepare budget road practice guess.\\nThank down sometimes factor.\\nLife health remain great edge culture. Me'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faker import Faker\n",
    "\n",
    "cache_content = Faker().text(max_nb_chars=15_000)\n",
    "cache_content[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any cached text to reference. The message you sent appears to be a collection of random sentences and phrases, but there isn't a specific \"cached text\" that I'm storing or referencing from our conversation.\n",
      "\n",
      "If you're asking about the first word of the text you just sent me, that would be \"Thank\" (from \"Thank down sometimes factor.\").\n",
      "\n",
      "Could you clarify what you mean by \"cached text\"? I'd be happy to help once I understand what specific text you're referring to.\n",
      "---\n",
      "Total Cost: 0.012538500000000001\n",
      "---\n",
      "Input Tokens: 17\n",
      "Output Tokens: 112\n",
      "Cache Write Tokens: 2882\n",
      "Cache Read Tokens: 0\n",
      "Total Tokens: 3011\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This example is modified from anthropic's prompt-caching.ipynb notebook\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "####\n",
    "from sik_llms import Anthropic\n",
    "from sik_llms.models_base import system_message, user_message, assistant_message\n",
    "\n",
    "client = Anthropic(\n",
    "    model_name=CLAUDE_MODEL,\n",
    "    temperature=0.1,\n",
    "    cache_content=cache_content,\n",
    ")\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "messages = [\n",
    "    system_message(\"You are a helpful assistant.\"),\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "]\n",
    "\n",
    "# first run should result in a cache-miss & write\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any cached text stored from our conversation. As I mentioned in my previous response, I don't maintain a cache of text that I can reference.\n",
      "\n",
      "If you're referring to the text in your first message, the second word would be \"down\" (from the phrase \"Thank down sometimes factor.\").\n",
      "\n",
      "However, I want to clarify that I'm not storing or caching any text - I'm simply reading and responding to what you've written in your messages. Could you help me understand what you mean by \"cached text\" so I can better assist you?\n",
      "---\n",
      "Total Cost: 0.0031206000000000003\n",
      "---\n",
      "Input Tokens: 142\n",
      "Output Tokens: 122\n",
      "Cache Write Tokens: 0\n",
      "Cache Read Tokens: 2882\n",
      "Total Tokens: 3146\n"
     ]
    }
   ],
   "source": [
    "# second run should result in a cache-hit & read\n",
    "messages = [\n",
    "    system_message(\"You are a helpful assistant.\"),\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "    assistant_message(response.response),\n",
    "    user_message(\"What is the second word of the cached text?\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock via OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sik_llms import OpenAI, user_message\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    server_url=os.getenv('BEDROCK_API_URL'),\n",
    "    api_key=os.getenv('BEDROCK_API_KEY'),\n",
    "    model_name='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "    user='bedrock-requires-user?',\n",
    ")\n",
    "response = client(messages=[user_message(\"What is the capital of France?\")])\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Log Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W (logprob: -0.61, prob: 0.54)\n",
      "aves (logprob: 0.00, prob: 1.00)\n",
      " whisper (logprob: -0.65, prob: 0.52)\n",
      " secrets (logprob: -0.06, prob: 0.94)\n",
      ", (logprob: -0.00, prob: 1.00)\n",
      "  \n",
      " (logprob: -0.00, prob: 1.00)\n",
      "End (logprob: -0.64, prob: 0.53)\n",
      "less (logprob: -0.00, prob: 1.00)\n",
      " blue (logprob: -0.16, prob: 0.85)\n",
      " ca (logprob: -3.16, prob: 0.04)\n",
      "resses (logprob: -0.05, prob: 0.95)\n",
      " shore (logprob: -0.21, prob: 0.81)\n",
      ", (logprob: -0.02, prob: 0.98)\n",
      "  \n",
      " (logprob: -0.00, prob: 1.00)\n",
      "Time (logprob: -0.97, prob: 0.38)\n",
      " flows (logprob: -1.29, prob: 0.27)\n",
      " like (logprob: -1.59, prob: 0.20)\n",
      " the (logprob: -0.08, prob: 0.92)\n",
      " tide (logprob: -0.02, prob: 0.98)\n",
      ". (logprob: -0.00, prob: 1.00)\n",
      "   (logprob: -1.91, prob: 0.15)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sik_llms import OpenAI, user_message, TextChunkEvent\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    model_name='gpt-4o-mini',  # logprobs don't seem to be supported by gpt-5 models\n",
    "    logprobs=True,\n",
    ")\n",
    "messages = [\n",
    "    user_message(\"Write a haiku about the ocean.\"),\n",
    "]\n",
    "async for response in client.stream(messages=messages):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        log_prob = response.logprob\n",
    "        prob = math.exp(log_prob)\n",
    "        print(f\"{response.content} (logprob: {log_prob:.2f}, prob: {prob:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=100 output_tokens=116 input_cost=2.4999999999999998e-05 output_cost=0.000232 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=2.346937666996382 response='A wooden boardwalk or footpath runs straight through tall green grasses across a flat wetland or meadow, leading toward a distant line of trees. Above is a wide blue sky streaked with wispy white clouds.'\n",
      "---\n",
      "A wooden boardwalk or footpath runs straight through tall green grasses across a flat wetland or meadow, leading toward a distant line of trees. Above is a wide blue sky streaked with wispy white clouds.\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import create_client, user_message, ImageContent\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "image = ImageContent.from_url(\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/320px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    ")\n",
    "\n",
    "client = create_client(model_name=OPENAI_MODEL)\n",
    "response = client(messages=[\n",
    "    user_message([\n",
    "        \"What's in this image? Describe it briefly.\",\n",
    "        image,\n",
    "    ]),\n",
    "])\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search w/ Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=2039 output_tokens=535 input_cost=0.006117 output_cost=0.008025000000000001 cache_write_tokens=0 cache_read_tokens=0 cache_write_cost=0.0 cache_read_cost=0.0 duration_seconds=12.173501041019335 response=\"Based on my search results, Michael Jordan's current net worth is estimated at $3.5 billion in 2025. This makes him the wealthiest former professional athlete in the world.\\n\\nThe sources of his massive wealth include:\\n\\n**Jordan Brand Partnership with Nike**: Jordan's contract was renegotiated into a 5% royalty on wholesale, giving him an estimated $350 million cash payout for 2024 alone. Jordan earned an estimated $300 million in 2024 without setting foot on the court, which topped the $260 million Cristiano Ronaldo earned as the world's highest-paid active athlete.\\n\\n**Charlotte Hornets Sale**: In 2023, he sold his majority stake for $3 billion, nearly tripling his investment from when he bought the franchise in 2010 for a reported $275 million.\\n\\n**NBA Career**: Jordan earned $90 million in salary during his NBA career, which is a small fraction of his current wealth.\\n\\n**Other Investments**: Jordan has diversified his portfolio with investments in DraftKings, where his 0.7% stake is now roughly worth $110 million as of April 2025, NASCAR team ownership, and various other business ventures.\\n\\nForbes designates him as the fourth-richest African-American and one of the richest celebrities, demonstrating how his business acumen has far exceeded his athletic earnings in building lasting wealth.\"\n",
      "---\n",
      "Based on my search results, Michael Jordan's current net worth is estimated at $3.5 billion in 2025. This makes him the wealthiest former professional athlete in the world.\n",
      "\n",
      "The sources of his massive wealth include:\n",
      "\n",
      "**Jordan Brand Partnership with Nike**: Jordan's contract was renegotiated into a 5% royalty on wholesale, giving him an estimated $350 million cash payout for 2024 alone. Jordan earned an estimated $300 million in 2024 without setting foot on the court, which topped the $260 million Cristiano Ronaldo earned as the world's highest-paid active athlete.\n",
      "\n",
      "**Charlotte Hornets Sale**: In 2023, he sold his majority stake for $3 billion, nearly tripling his investment from when he bought the franchise in 2010 for a reported $275 million.\n",
      "\n",
      "**NBA Career**: Jordan earned $90 million in salary during his NBA career, which is a small fraction of his current wealth.\n",
      "\n",
      "**Other Investments**: Jordan has diversified his portfolio with investments in DraftKings, where his 0.7% stake is now roughly worth $110 million as of April 2025, NASCAR team ownership, and various other business ventures.\n",
      "\n",
      "Forbes designates him as the fourth-richest African-American and one of the richest celebrities, demonstrating how his business acumen has far exceeded his athletic earnings in building lasting wealth.\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import create_client, user_message\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = create_client(model_name=CLAUDE_MODEL, web_search=True)\n",
    "response = client(messages=[user_message(\"What the current net worth of Michael Jordan?\")])\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
