{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanekercheval/repos/sik-llms/src\n"
     ]
    }
   ],
   "source": [
    "%cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = 'gpt-5-mini'\n",
    "CLAUDE_MODEL = 'claude-sonnet-4-latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.openai.OpenAI at 0x1128fa7b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For \"registered\" clients (via `@Client.register`), the client\n",
    "# can be created with `create_client` by passing in the model name.\n",
    "from sik_llms import create_client\n",
    "\n",
    "client = create_client(\n",
    "    model_name=OPENAI_MODEL,\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.openai.OpenAI at 0x112f88cd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, the client can be directly instantiated\n",
    "from sik_llms import OpenAI\n",
    "client = OpenAI(\n",
    "    model_name=OPENAI_MODEL,\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.anthropic.Anthropic at 0x112fe5a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, the client can be directly instantiated\n",
    "from sik_llms import Anthropic\n",
    "client = Anthropic(\n",
    "    model_name=CLAUDE_MODEL,\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user', 'content': 'What is the capital of France?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sik_llms import create_client, user_message, TextChunkEvent\n",
    "\n",
    "client = create_client(\n",
    "    model_name=OPENAI_MODEL,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the capital of France?\")\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Synchronously via `__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=13 output_tokens=16 input_cost=3.25e-06 output_cost=3.2e-05 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=1.3067527090006479 response='The capital of France is Paris.'\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = client(messages=[message])\n",
    "print(response)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Aynchronously via `run_async`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=13 output_tokens=80 input_cost=3.25e-06 output_cost=0.00015999999999999999 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=1.446224584000447 response='The capital of France is Paris.'\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = await client.run_async(messages=[message])\n",
    "print(response)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Asynchronously via `stream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "async for response in client.stream(messages=[message]):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        print(response.content, end=\"\")\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextChunkEvent(content='The', logprob=None),\n",
       " TextChunkEvent(content=' capital', logprob=None),\n",
       " TextChunkEvent(content=' of', logprob=None),\n",
       " TextChunkEvent(content=' France', logprob=None),\n",
       " TextChunkEvent(content=' is', logprob=None),\n",
       " TextChunkEvent(content=' Paris', logprob=None),\n",
       " TextChunkEvent(content='.', logprob=None),\n",
       " TextResponse(input_tokens=13, output_tokens=80, input_cost=3.25e-06, output_cost=0.00015999999999999999, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=1.5054410000011558, response='The capital of France is Paris.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate multiple responses from a single input via `sample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['42', '78546', '47']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TextResponse(input_tokens=16, output_tokens=1, input_cost=2.4e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.5566354580005282, response='42'),\n",
       " TextResponse(input_tokens=16, output_tokens=2, input_cost=2.4e-06, output_cost=1.2e-06, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.6124812919988472, response='78546'),\n",
       " TextResponse(input_tokens=16, output_tokens=1, input_cost=2.4e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.5687354159999813, response='47')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = create_client(model_name='gpt-4o-mini', temperature=1.5)\n",
    "responses = await client.sample(\n",
    "    messages=[user_message('Generate a random number. Return only the number')],\n",
    "    n=3,\n",
    ")\n",
    "print([r.response for r in responses])\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate multiple responses from multiple inputs via `generate_multiple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paris', 'Rome']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TextResponse(input_tokens=19, output_tokens=74, input_cost=4.749999999999999e-06, output_cost=0.000148, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=2.909112332999939, response='Paris'),\n",
       " TextResponse(input_tokens=19, output_tokens=74, input_cost=4.749999999999999e-06, output_cost=0.000148, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=1.7156433749987627, response='Rome')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = create_client(model_name=OPENAI_MODEL)\n",
    "responses = await client.generate_multiple(\n",
    "    messages=[\n",
    "        [user_message(\"What is the capital of France? Return only the city name.\")],\n",
    "        [user_message(\"What is the capital of Italy? Return only the city name.\")],\n",
    "    ],\n",
    ")\n",
    "print([r.response for r in responses])\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate `sample_n` responses from multiple inputs via `generate_multiple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['47', '37', '42', '73', '57']\n",
      "['147', '143', '163', '157', '157']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.4945137500017154, response='47'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.6733545419992879, response='37'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.7809303750000254, response='42'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.493597624999893, response='73'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.5513699999992241, response='57')],\n",
       " [TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.5309028329993453, response='147'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.5325438330000907, response='143'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.5145132500001637, response='163'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.6044779160001781, response='157'),\n",
       "  TextResponse(input_tokens=23, output_tokens=1, input_cost=3.45e-06, output_cost=6e-07, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=0.5346527920009976, response='157')]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = create_client(model_name='gpt-4o-mini', temperature=1.5)\n",
    "responses_set = await client.generate_multiple(\n",
    "    messages=[\n",
    "        [user_message(\"Pick a random number between 1 and 100. Return only the number.\")],\n",
    "        [user_message(\"Pick a random number between 100 and 200. Return only the number.\")],\n",
    "    ],\n",
    "    sample_n=5,\n",
    ")\n",
    "for responses in responses_set:\n",
    "    print([r.response for r in responses])\n",
    "responses_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Functions/Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=141 output_tokens=153 input_cost=3.5249999999999996e-05 output_cost=0.000306 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=2.9399389999998675 tool_prediction=ToolPrediction(name='get_weather', arguments={'location': 'Paris, France'}, call_id='call_Togxm6Iw3m3dhxGHrLEoKeMc') message=None\n",
      "---\n",
      "name='get_weather' arguments={'location': 'Paris, France'} call_id='call_Togxm6Iw3m3dhxGHrLEoKeMc'\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    Tool, Parameter, RegisteredClients,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name='get_weather',\n",
    "    description=\"Get the weather for a location.\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='location',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description='The city and country for weather info.',\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    client_type=RegisteredClients.OPENAI_TOOLS,\n",
    "    model_name=OPENAI_MODEL,\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the weather in Paris?\")\n",
    "response = await client.run_async(messages=[message])\n",
    "# or `response = client(messages=[message])` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.tool_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude Functions/Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=402 output_tokens=40 input_cost=0.001206 output_cost=0.0006000000000000001 cache_write_tokens=0 cache_read_tokens=0 cache_write_cost=0.0 cache_read_cost=0.0 duration_seconds=2.3683890410011372 tool_prediction=ToolPrediction(name='get_weather', arguments={'location': 'Paris, France'}, call_id='toolu_01GoWnSxbeS9bTUZKBN1FQue') message=None\n",
      "---\n",
      "name='get_weather' arguments={'location': 'Paris, France'} call_id='toolu_01GoWnSxbeS9bTUZKBN1FQue'\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    Tool, Parameter, RegisteredClients,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name='get_weather',\n",
    "    description=\"Get the weather for a location.\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='location',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description='The city and country for weather info.',\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    client_type=RegisteredClients.ANTHROPIC_TOOLS,\n",
    "    model_name=CLAUDE_MODEL,\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the weather in Paris?\")\n",
    "response = await client.run_async(messages=[message])\n",
    "# or `response = client(messages=[message])` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.tool_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs via OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=90 output_tokens=350 input_cost=2.2499999999999998e-05 output_cost=0.0007 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=4.061760750000758 parsed=CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob']) refusal=None\n",
      "---\n",
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):  # noqa: D101\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "client = create_client(\n",
    "    model_name=OPENAI_MODEL,\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "messages=[\n",
    "    system_message(\"Extract the event information.\"),\n",
    "    user_message(\"Alice and Bob are going to a science fair on Friday.\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "# or `response = client(messages=messages)` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs via Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=442 output_tokens=78 input_cost=0.0013260000000000001 output_cost=0.00117 cache_write_tokens=None cache_read_tokens=None cache_write_cost=None cache_read_cost=None duration_seconds=2.2905782089983404 parsed=CalendarEvent(name='Science fair', date='Friday', participants=['Alice', 'Bob']) refusal=None\n",
      "---\n",
      "name='Science fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):  # noqa: D101\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "client = create_client(\n",
    "    model_name=CLAUDE_MODEL,\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "messages=[\n",
    "    system_message(\"Extract the event information.\"),\n",
    "    user_message(\"Alice and Bob are going to a science fair on Friday.\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "# or `response = client(messages=messages)` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning via OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the expression 1 + 2 + (3 * 4) + (5 * 6), first calculate the multiplications inside the parentheses:\n",
      "\n",
      "• 3 * 4 = 12\n",
      "• 5 * 6 = 30\n",
      "\n",
      "Now substitute these values back into the expression:\n",
      "\n",
      "1 + 2 + 12 + 30\n",
      "\n",
      "Then, proceed with the addition:\n",
      "\n",
      "1 + 2 = 3  \n",
      "3 + 12 = 15  \n",
      "15 + 30 = 45\n",
      "\n",
      "So, the final answer is 45."
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    TextChunkEvent, TextResponse, ReasoningEffort,\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    model_name='o3-mini',\n",
    "    reasoning_effort=ReasoningEffort.MEDIUM,\n",
    ")\n",
    "messages=[user_message(\"What is 1 + 2 + (3 * 4) + (5 * 6)?\")]\n",
    "summary = None\n",
    "async for response in client.stream(messages=messages):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        print(response.content, end=\"\")\n",
    "    elif isinstance(response, TextResponse):\n",
    "        summary = response\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response type: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextResponse(input_tokens=27, output_tokens=254, input_cost=2.97e-05, output_cost=0.0011176, cache_write_tokens=None, cache_read_tokens=0, cache_write_cost=None, cache_read_cost=0.0, duration_seconds=2.4380913749992033, response='To solve the expression 1 + 2 + (3 * 4) + (5 * 6), first calculate the multiplications inside the parentheses:\\n\\n• 3 * 4 = 12\\n• 5 * 6 = 30\\n\\nNow substitute these values back into the expression:\\n\\n1 + 2 + 12 + 30\\n\\nThen, proceed with the addition:\\n\\n1 + 2 = 3  \\n3 + 12 = 15  \\n15 + 30 = 45\\n\\nSo, the final answer is 45.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning via Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[THINKING]\n",
      "I need to calculate this step by step, following the order of operations (PEMDAS/BODMAS).\n",
      "\n",
      "The expression is: 1 + 2 + (3 * 4) + (5 * 6)\n",
      "\n",
      "First, I'll handle the operations in parentheses:\n",
      "- (3 * 4) = 12\n",
      "- (5 * 6) = 30\n",
      "\n",
      "Now the expression becomes: 1 + 2 + 12 + 30\n",
      "\n",
      "Now I can add from left to right:\n",
      "1 + 2 = 3\n",
      "3 + 12 = 15\n",
      "15 + 30 = 45\n",
      "\n",
      "[TEXT]\n",
      "I'll solve this step by step, following the order of operations.\n",
      "\n",
      "1 + 2 + (3 * 4) + (5 * 6)\n",
      "\n",
      "First, I'll calculate the multiplications in parentheses:\n",
      "- (3 * 4) = 12\n",
      "- (5 * 6) = 30\n",
      "\n",
      "Now the expression becomes:\n",
      "1 + 2 + 12 + 30\n",
      "\n",
      "Adding from left to right:\n",
      "1 + 2 + 12 + 30 = 45\n",
      "\n",
      "The answer is **45**."
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    TextChunkEvent, ThinkingChunkEvent,\n",
    "    TextResponse, ReasoningEffort,\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    model_name=CLAUDE_MODEL,\n",
    "    reasoning_effort=ReasoningEffort.MEDIUM,\n",
    ")\n",
    "messages=[user_message(\"What is 1 + 2 + (3 * 4) + (5 * 6)?\")]\n",
    "summary = None\n",
    "\n",
    "current_type = None\n",
    "async for response in client.stream(messages=messages):\n",
    "    is_text_chunk = isinstance(response, TextChunkEvent)\n",
    "    is_thinking_chunk = isinstance(response, ThinkingChunkEvent)\n",
    "    is_summary = isinstance(response, TextResponse)\n",
    "\n",
    "    if is_text_chunk or is_thinking_chunk:\n",
    "        if type(response) is not current_type:\n",
    "            print(f\"\\n\\n[{'THINKING' if is_thinking_chunk else 'TEXT'}]\")\n",
    "            current_type = type(response)\n",
    "        print(response.content, end=\"\")\n",
    "    elif isinstance(response, TextResponse):\n",
    "        summary = response\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response type: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReasoningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUESTION]: I'm planning a trip to New York and Miami. What's the weather like in both cities? Also, if I have a budget of $2400 for a 6-day trip, how much can I spend per day?\n",
      "\n",
      "\n",
      "--- Iteration 1\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "I need the current weather for New York. I'll call the weather tool to get up-to-date conditions.\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: get_weather`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"location\": \"New York\",\n",
      "  \"units\": \"F\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: get_weather`\n",
      "Result: {'New York': '68°F'}\n",
      "\n",
      "--- Iteration 2\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "I have New York's current temperature (68°F). Next I should get Miami's current weather (use same units for consistency) and then compute the per-day budget for the 6-day trip ($2400 ÷ 6). I'll call the weather tool for Miami now.\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: get_weather`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"location\": \"Miami\",\n",
      "  \"units\": \"F\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: get_weather`\n",
      "Result: {'Miami': '85°F'}\n",
      "\n",
      "--- Iteration 3\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "I have the current temperatures from the tool: New York 68°F and Miami 85°F. Interpreting that: New York is mild/comfortable, Miami is quite warm/hot. For the budget: $2400 ÷ 6 days = $400 per day, so you can spend up to $400 each day on average.\n",
      "\n",
      "[FINAL RESPONSE]:\n",
      "Summary from the reasoning above:\n",
      "\n",
      "- New York: 68°F (≈20°C) — mild/comfortable at the moment.  \n",
      "- Miami: 85°F (≈29°C) — warm to hot right now.\n",
      "\n",
      "Budget:\n",
      "- $2,400 ÷ 6 days = $400 per day.\n",
      "\n",
      "Notes: the temperatures reported are current snapshots (no precipitation/wind details) and can change; for budgeting, consider setting aside a small buffer (e.g., 10–20%) for taxes, tips, or unexpected expenses.\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sik_llms.models_base import (\n",
    "    Tool, Parameter, ThinkingEvent, ToolPredictionEvent,\n",
    "    ToolResultEvent, TextChunkEvent, ErrorEvent, TextResponse,\n",
    ")\n",
    "from sik_llms.reasoning_agent import ReasoningAgent\n",
    "\n",
    "####\n",
    "# Define the tool functions\n",
    "####\n",
    "async def calculator(expression: str) -> str:\n",
    "    \"\"\"Execute calculator tool.\"\"\"\n",
    "    try:\n",
    "        # Only allow simple arithmetic for safety\n",
    "        allowed_chars = set('0123456789+-*/() .')\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return \"Error: Invalid characters in expression\"\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e!s}\"\n",
    "\n",
    "\n",
    "async def weather(location: str, units: str) -> str:\n",
    "    \"\"\"Mock weather tool - returns fake data.\"\"\"\n",
    "    # Return mock weather data\n",
    "    weather_data = {\n",
    "        'New York': '68',\n",
    "        'San Francisco': '62',\n",
    "        'Miami': '85',\n",
    "        'Chicago': '55',\n",
    "        'Los Angeles': '75',\n",
    "    }\n",
    "    for city in weather_data:  # noqa: PLC0206\n",
    "        if city.lower() in location.lower():\n",
    "            temp = weather_data[city]\n",
    "            if units == 'C':\n",
    "                # C = (°F - 32) x (5/9)\n",
    "                temp = round((temp - 32) * 5 / 9)\n",
    "            return {location: f\"{temp}°{units}\"}\n",
    "    return None\n",
    "\n",
    "####\n",
    "# Define tool objects\n",
    "####\n",
    "calculator_tool = Tool(\n",
    "    name='calculator',\n",
    "    description=\"Perform mathematical calculations\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='expression',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description=\"The mathematical expression to evaluate (e.g., '2 + 2', '5 * 10')\",\n",
    "        ),\n",
    "    ],\n",
    "    func=calculator,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather for a location\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name=\"location\",\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description=\"The name of the city (e.g., 'San Francisco', 'New York', 'London')\",\n",
    "        ),\n",
    "        Parameter(\n",
    "            name='units',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description=\"The units for temperature\",\n",
    "            valid_values=['F', 'C'],\n",
    "        ),\n",
    "    ],\n",
    "    func=weather,\n",
    ")\n",
    "\n",
    "# Create the reasoning agent\n",
    "agent = ReasoningAgent(\n",
    "    model_name=OPENAI_MODEL,  # You can change this to other models\n",
    "    tools=[calculator_tool, weather_tool],\n",
    "    max_iterations=10,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "question = \"I'm planning a trip to New York and Miami. What's the weather like in both cities? Also, if I have a budget of $2400 for a 6-day trip, how much can I spend per day?\"  # noqa: E501\n",
    "# Run the agent and collect the results\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "print(f\"[QUESTION]: {question}\\n\")\n",
    "\n",
    "current_iteration = 0\n",
    "\n",
    "async for event in agent.stream(messages):\n",
    "    if isinstance(event, ThinkingEvent):\n",
    "        if hasattr(event, 'iteration') and event.iteration != current_iteration:\n",
    "            current_iteration = event.iteration\n",
    "            print(f\"\\n--- Iteration {current_iteration}\\n\")\n",
    "        if event.content:\n",
    "            print(f\"\\n[THINKING]:\\n{event.content}\")\n",
    "\n",
    "    elif isinstance(event, ToolPredictionEvent):\n",
    "        print(\"\\n[TOOL PREDICTION]:\")\n",
    "        print(f\"Tool: {event.name}`\")\n",
    "        print(f\"Parameters: \\n```json\\n{json.dumps(event.arguments, indent=2)}\\n```\")\n",
    "\n",
    "    elif isinstance(event, ToolResultEvent):\n",
    "        print(\"\\n[TOOL RESULT]:\")\n",
    "        print(f\"Tool: {event.name}`\")\n",
    "        print(f\"Result: {event.result}\")\n",
    "\n",
    "    elif isinstance(event, ErrorEvent):\n",
    "        print(\"\\n[ERROR]:\")\n",
    "        print(f\"Error: {event.content}\")\n",
    "\n",
    "    elif isinstance(event, TextChunkEvent):\n",
    "        # For streaming text generation\n",
    "        if current_iteration  >= 0:  # Only print once for the header\n",
    "            print(\"\\n[FINAL RESPONSE]:\")\n",
    "            current_iteration = -1  # Set to an impossible iteration to avoid repeating\n",
    "\n",
    "        print(event.content, end=\"\")\n",
    "\n",
    "    elif isinstance(event, TextResponse):\n",
    "        # Print nothing here as we've already streamed the response\n",
    "        pass\n",
    "\n",
    "# Just for notebook display cleanliness\n",
    "print(\"\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens: 3389\n",
      "Output Tokens: 3447\n",
      "Total Cost: 0.007741249999999999\n",
      "Duration: 41.64 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input Tokens: {event.input_tokens}\")\n",
    "print(f\"Output Tokens: {event.output_tokens}\")\n",
    "print(f\"Total Cost: {event.total_cost}\")\n",
    "print(f\"Duration: {event.duration_seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic - `cache_control` parameter in `system_message`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She civil staff but time majority onto claim. Economy if message physical someone choice off.\\nAround left gas drop drop matter. Sound compare charge nature drug support art why. Pass administration on'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faker import Faker\n",
    "\n",
    "cache_content = Faker().text(max_nb_chars=15_000)\n",
    "cache_content[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first word of the cached text is \"She\".\n",
      "---\n",
      "Total Cost: 0.01113975\n",
      "---\n",
      "Input Tokens: 17\n",
      "Output Tokens: 14\n",
      "Cache Write Tokens: 2901\n",
      "Cache Read Tokens: 0\n",
      "Total Tokens: 2932\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This example is modified from anthropic's prompt-caching.ipynb notebook\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "####\n",
    "from sik_llms import Anthropic\n",
    "from sik_llms.models_base import system_message, user_message, assistant_message\n",
    "\n",
    "client = Anthropic(\n",
    "    model_name=CLAUDE_MODEL,\n",
    "    temperature=0.1,\n",
    ")\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "system_messages = [\n",
    "    system_message(\"You are a helpful assistant.\"),\n",
    "    system_message(\n",
    "        cache_content,\n",
    "        cache_control={'type': 'ephemeral'},\n",
    "    ),\n",
    "]\n",
    "messages = [\n",
    "    *system_messages,\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "]\n",
    "\n",
    "# first run should result in a cache-miss & write\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second word of the cached text is \"civil\".\n",
      "---\n",
      "Total Cost: 0.0012123\n",
      "---\n",
      "Input Tokens: 44\n",
      "Output Tokens: 14\n",
      "Cache Write Tokens: 0\n",
      "Cache Read Tokens: 2901\n",
      "Total Tokens: 2959\n"
     ]
    }
   ],
   "source": [
    "# second run should result in a cache-hit & read\n",
    "messages = [\n",
    "    *system_messages,\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "    assistant_message(response.response),\n",
    "    user_message(\"What is the second word of the cached text?\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic - `cache_content` parameter in `__init__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First thank research site tell. Whatever exist special defense knowledge effort.\\nApproach citizen whether admit issue party hotel moment. Quite return stage tell sit affect plan.\\nAlso cup including st'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faker import Faker\n",
    "\n",
    "cache_content = Faker().text(max_nb_chars=15_000)\n",
    "cache_content[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first word of the cached text is \"First\".\n",
      "---\n",
      "Total Cost: 0.011057250000000001\n",
      "---\n",
      "Input Tokens: 17\n",
      "Output Tokens: 14\n",
      "Cache Write Tokens: 2879\n",
      "Cache Read Tokens: 0\n",
      "Total Tokens: 2910\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This example is modified from anthropic's prompt-caching.ipynb notebook\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "####\n",
    "from sik_llms import Anthropic\n",
    "from sik_llms.models_base import system_message, user_message, assistant_message\n",
    "\n",
    "client = Anthropic(\n",
    "    model_name=CLAUDE_MODEL,\n",
    "    temperature=0.1,\n",
    "    cache_content=cache_content,\n",
    ")\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "messages = [\n",
    "    system_message(\"You are a helpful assistant.\"),\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "]\n",
    "\n",
    "# first run should result in a cache-miss & write\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second word of the cached text is \"thank\".\n",
      "---\n",
      "Total Cost: 0.0012057\n",
      "---\n",
      "Input Tokens: 44\n",
      "Output Tokens: 14\n",
      "Cache Write Tokens: 0\n",
      "Cache Read Tokens: 2879\n",
      "Total Tokens: 2937\n"
     ]
    }
   ],
   "source": [
    "# second run should result in a cache-hit & read\n",
    "messages = [\n",
    "    system_message(\"You are a helpful assistant.\"),\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "    assistant_message(response.response),\n",
    "    user_message(\"What is the second word of the cached text?\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock via OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sik_llms import OpenAI, user_message\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    server_url=os.getenv('BEDROCK_API_URL'),\n",
    "    api_key=os.getenv('BEDROCK_API_KEY'),\n",
    "    model_name='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "    user='bedrock-requires-user?',\n",
    ")\n",
    "response = client(messages=[user_message(\"What is the capital of France?\")])\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Log Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W (logprob: -0.44, prob: 0.65)\n",
      "aves (logprob: 0.00, prob: 1.00)\n",
      " kiss (logprob: -1.04, prob: 0.35)\n",
      " sandy (logprob: -1.17, prob: 0.31)\n",
      " shores (logprob: -0.35, prob: 0.71)\n",
      ", (logprob: -0.00, prob: 1.00)\n",
      "  \n",
      " (logprob: -0.00, prob: 1.00)\n",
      "Wh (logprob: -0.10, prob: 0.91)\n",
      "ispers (logprob: -0.00, prob: 1.00)\n",
      " of (logprob: -0.00, prob: 1.00)\n",
      " the (logprob: -0.08, prob: 0.92)\n",
      " deep (logprob: -0.05, prob: 0.95)\n",
      " blue (logprob: -1.35, prob: 0.26)\n",
      " call (logprob: -0.21, prob: 0.81)\n",
      ", (logprob: -0.01, prob: 0.99)\n",
      "  \n",
      " (logprob: -0.00, prob: 1.00)\n",
      "End (logprob: -0.28, prob: 0.76)\n",
      "less (logprob: -0.00, prob: 1.00)\n",
      " tales (logprob: -3.03, prob: 0.05)\n",
      " unfold (logprob: -0.23, prob: 0.80)\n",
      ". (logprob: -0.00, prob: 1.00)\n",
      "   (logprob: -2.58, prob: 0.08)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sik_llms import OpenAI, user_message, TextChunkEvent\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    model_name='gpt-4o-mini',  # logprobs don't seem to be supported by gpt-5 models\n",
    "    logprobs=True,\n",
    ")\n",
    "messages = [\n",
    "    user_message(\"Write a haiku about the ocean.\"),\n",
    "]\n",
    "async for response in client.stream(messages=messages):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        log_prob = response.logprob\n",
    "        prob = math.exp(log_prob)\n",
    "        print(f\"{response.content} (logprob: {log_prob:.2f}, prob: {prob:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=100 output_tokens=121 input_cost=2.4999999999999998e-05 output_cost=0.000242 cache_write_tokens=None cache_read_tokens=0 cache_write_cost=None cache_read_cost=0.0 duration_seconds=1.8928221669993945 response='A wooden boardwalk or footpath runs straight through a wide green meadow or marsh of tall grasses, leading toward the horizon. The scene is under a bright blue sky with scattered white clouds, giving a peaceful, open‑landscape feel.'\n",
      "---\n",
      "A wooden boardwalk or footpath runs straight through a wide green meadow or marsh of tall grasses, leading toward the horizon. The scene is under a bright blue sky with scattered white clouds, giving a peaceful, open‑landscape feel.\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import create_client, user_message, ImageContent\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "image = ImageContent.from_url(\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/320px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    ")\n",
    "\n",
    "client = create_client(model_name=OPENAI_MODEL)\n",
    "response = client(messages=[\n",
    "    user_message([\n",
    "        \"What's in this image? Describe it briefly.\",\n",
    "        image,\n",
    "    ]),\n",
    "])\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search w/ Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=2039 output_tokens=542 input_cost=0.006117 output_cost=0.00813 cache_write_tokens=0 cache_read_tokens=0 cache_write_cost=0.0 cache_read_cost=0.0 duration_seconds=12.750808541000879 response=\"Based on the search results, Michael Jordan's net worth in 2025 is estimated at $3.5 billion according to Forbes, making him the wealthiest former professional athlete in the world.\\n\\nThis massive fortune comes from several key sources:\\n\\n**Jordan Brand Royalties**: Jordan's contract was renegotiated into a 5% royalty on wholesale, giving him an estimated $350 million cash payout for 2024 alone. In 2024 Jordan Brand sold about $7 billion, 14% of Nike's total revenue.\\n\\n**Charlotte Hornets Sale**: In 2023, he sold his majority stake for $3 billion, nearly tripling his investment from when he bought a majority stake in the team for $175 million back in 2010.\\n\\n**NBA Career Earnings**: Jordan earned $90 million in salary during his NBA career, which while substantial, represents only a small fraction of his total wealth.\\n\\n**Other Investments**: Jordan has diversified his portfolio with investments in DraftKings (0.7% stake worth roughly $110 million as of April 2025) and a 10% founder's stake in Cincoro Tequila.\\n\\nRemarkably, Jordan earned an estimated $300 million in 2024 without setting foot on the court, demonstrating how his business empire continues to generate massive wealth decades after his retirement from basketball.\"\n",
      "---\n",
      "Based on the search results, Michael Jordan's net worth in 2025 is estimated at $3.5 billion according to Forbes, making him the wealthiest former professional athlete in the world.\n",
      "\n",
      "This massive fortune comes from several key sources:\n",
      "\n",
      "**Jordan Brand Royalties**: Jordan's contract was renegotiated into a 5% royalty on wholesale, giving him an estimated $350 million cash payout for 2024 alone. In 2024 Jordan Brand sold about $7 billion, 14% of Nike's total revenue.\n",
      "\n",
      "**Charlotte Hornets Sale**: In 2023, he sold his majority stake for $3 billion, nearly tripling his investment from when he bought a majority stake in the team for $175 million back in 2010.\n",
      "\n",
      "**NBA Career Earnings**: Jordan earned $90 million in salary during his NBA career, which while substantial, represents only a small fraction of his total wealth.\n",
      "\n",
      "**Other Investments**: Jordan has diversified his portfolio with investments in DraftKings (0.7% stake worth roughly $110 million as of April 2025) and a 10% founder's stake in Cincoro Tequila.\n",
      "\n",
      "Remarkably, Jordan earned an estimated $300 million in 2024 without setting foot on the court, demonstrating how his business empire continues to generate massive wealth decades after his retirement from basketball.\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import create_client, user_message\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = create_client(model_name=CLAUDE_MODEL, web_search=True)\n",
    "response = client(messages=[user_message(\"What the current net worth of Michael Jordan?\")])\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
