{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanekercheval/repos/sik-llms/src\n"
     ]
    }
   ],
   "source": [
    "%cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.openai.OpenAI at 0x1090aa660>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For \"registered\" clients (via `@Client.register`), the client\n",
    "# can be created with `create_client` by passing in the model name.\n",
    "from sik_llms import create_client\n",
    "\n",
    "client = create_client(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.openai.OpenAI at 0x10935b390>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, the client can be directly instantiated\n",
    "from sik_llms import OpenAI\n",
    "client = OpenAI(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.anthropic.Anthropic at 0x1097f12b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, the client can be directly instantiated\n",
    "from sik_llms import Anthropic\n",
    "client = Anthropic(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user', 'content': 'What is the capital of France?'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sik_llms import create_client, user_message, TextChunkEvent\n",
    "\n",
    "client = create_client(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the capital of France?\")\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Synchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseSummary(input_tokens=14, output_tokens=7, input_cost=2.1e-06, output_cost=4.2e-06, duration_seconds=0.6609218120574951, response='The capital of France is Paris.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client(messages=[message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "async for response in client.run_async(messages=[message]):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        print(response.content, end=\"\")\n",
    "        responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=14 output_tokens=7 input_cost=2.1e-06 output_cost=4.2e-06 duration_seconds=0.30423903465270996 response='The capital of France is Paris.'\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseSummary(input_tokens=14, output_tokens=50, input_cost=4.2000000000000004e-05, output_cost=0.00075, duration_seconds=1.310349941253662, response='The capital of France is Paris. Paris is not only the capital but also the largest city in France, known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = create_client(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    temperature=0.1,\n",
    ")\n",
    "response = client(messages=[user_message(\"What is the capital of France?\")])\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Functions/Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=60 output_tokens=17 input_cost=9e-06 output_cost=1.0199999999999999e-05 duration_seconds=0.5924029350280762 tool_prediction=ToolPrediction(name='get_weather', arguments={'location': 'Paris, France'}, call_id='call_4VhFoGud6HLi2HhIKQQtq4B5') message=None\n",
      "---\n",
      "name='get_weather' arguments={'location': 'Paris, France'} call_id='call_4VhFoGud6HLi2HhIKQQtq4B5'\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import Tool, Parameter, RegisteredClients\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name='get_weather',\n",
    "    description=\"Get the weather for a location.\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='location',\n",
    "            type='string',\n",
    "            required=True,\n",
    "            description='The city and country for weather info.',\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    client_type=RegisteredClients.OPENAI_TOOLS,\n",
    "    model_name='gpt-4o-mini',\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the weather in Paris?\")\n",
    "response = await client.run_async(messages=[message])\n",
    "# or `response = client(messages=[message])` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.tool_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude Functions/Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=394 output_tokens=40 input_cost=0.001182 output_cost=0.0006000000000000001 duration_seconds=1.670184850692749 tool_prediction=ToolPrediction(name='get_weather', arguments={'location': 'Paris, France'}, call_id='toolu_01QRGTT9RmTcx9dzTtdzKAwJ') message=None\n",
      "---\n",
      "name='get_weather' arguments={'location': 'Paris, France'} call_id='toolu_01QRGTT9RmTcx9dzTtdzKAwJ'\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client,\n",
    "    user_message,\n",
    "    Tool,\n",
    "    Parameter,\n",
    "    RegisteredClients,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name='get_weather',\n",
    "    description=\"Get the weather for a location.\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='location',\n",
    "            type='string',\n",
    "            required=True,\n",
    "            description='The city and country for weather info.',\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    client_type=RegisteredClients.ANTHROPIC_TOOLS,\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the weather in Paris?\")\n",
    "response = await client.run_async(messages=[message])\n",
    "# or `response = client(messages=[message])` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.tool_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs via OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=92 output_tokens=18 input_cost=1.38e-05 output_cost=1.08e-05 duration_seconds=0.7497479915618896 response=StructuredOutputResponse(parsed=CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob']), refusal=None)\n",
      "---\n",
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):  # noqa: D101\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "client = create_client(\n",
    "    model_name='gpt-4o-mini',\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "messages=[\n",
    "    system_message(\"Extract the event information.\"),\n",
    "    user_message(\"Alice and Bob are going to a science fair on Friday.\"),\n",
    "]\n",
    "response = client(messages=messages)\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.response.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs via Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=458 output_tokens=78 input_cost=0.001374 output_cost=0.00117 duration_seconds=1.7810969352722168 response=StructuredOutputResponse(parsed=CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob']), refusal=None)\n",
      "---\n",
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):  # noqa: D101\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "client = create_client(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "messages=[\n",
    "    system_message(\"Extract the event information.\"),\n",
    "    user_message(\"Alice and Bob are going to a science fair on Friday.\"),\n",
    "]\n",
    "response = client(messages=messages)\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.response.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning via OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down step by step:\n",
      "\n",
      "1. First, calculate the multiplications:\n",
      "   - 3 * 4 = 12\n",
      "   - 5 * 6 = 30\n",
      "\n",
      "2. Now, substitute those back into the expression:\n",
      "   - 1 + 2 + 12 + 30\n",
      "\n",
      "3. Finally, add them together:\n",
      "   - 1 + 2 = 3\n",
      "   - 3 + 12 = 15\n",
      "   - 15 + 30 = 45\n",
      "\n",
      "So, the answer is 45."
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client,\n",
    "    user_message,\n",
    "    TextChunkEvent,\n",
    "    ResponseSummary,\n",
    "    ReasoningEffort,\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    model_name='o3-mini',\n",
    "    reasoning_effort=ReasoningEffort.MEDIUM,\n",
    ")\n",
    "messages=[user_message(\"What is 1 + 2 + (3 * 4) + (5 * 6)?\")]\n",
    "summary = None\n",
    "async for response in client.run_async(messages=messages):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        print(response.content, end=\"\")\n",
    "    elif isinstance(response, ResponseSummary):\n",
    "        summary = response\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response type: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseSummary(input_tokens=28, output_tokens=113, input_cost=3.08e-05, output_cost=0.0004972, duration_seconds=2.5976948738098145, response=\"Let's break it down step by step:\\n\\n1. First, calculate the multiplications:\\n   - 3 * 4 = 12\\n   - 5 * 6 = 30\\n\\n2. Now, substitute those back into the expression:\\n   - 1 + 2 + 12 + 30\\n\\n3. Finally, add them together:\\n   - 1 + 2 = 3\\n   - 3 + 12 = 15\\n   - 15 + 30 = 45\\n\\nSo, the answer is 45.\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning via Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[THINKING]\n",
      "I need to calculate 1 + 2 + (3 * 4) + (5 * 6).\n",
      "\n",
      "Let me break it down step by step, following the order of operations (PEMDAS: Parentheses, Exponents, Multiplication/Division, Addition/Subtraction).\n",
      "\n",
      "First, let's calculate the expressions in parentheses:\n",
      "- (3 * 4) = 12\n",
      "- (5 * 6) = 30\n",
      "\n",
      "Now, our expression becomes: 1 + 2 + 12 + 30\n",
      "\n",
      "Adding these numbers:\n",
      "1 + 2 = 3\n",
      "3 + 12 = 15\n",
      "15 + 30 = 45\n",
      "\n",
      "So, 1 + 2 + (3 * 4) + (5 * 6) = 45.\n",
      "\n",
      "[TEXT]\n",
      "To solve this expression, I'll follow the order of operations:\n",
      "\n",
      "1) First, calculate the expressions in parentheses:\n",
      "   - (3 * 4) = 12\n",
      "   - (5 * 6) = 30\n",
      "\n",
      "2) Now add all terms together:\n",
      "   1 + 2 + 12 + 30 = 45\n",
      "\n",
      "The answer is 45."
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client,\n",
    "    user_message,\n",
    "    TextChunkEvent,\n",
    "    ThinkingChunkEvent,\n",
    "    ResponseSummary,\n",
    "    ReasoningEffort,\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    reasoning_effort=ReasoningEffort.MEDIUM,\n",
    ")\n",
    "messages=[user_message(\"What is 1 + 2 + (3 * 4) + (5 * 6)?\")]\n",
    "summary = None\n",
    "\n",
    "current_type = None\n",
    "async for response in client.run_async(messages=messages):\n",
    "    is_text_chunk = isinstance(response, TextChunkEvent)\n",
    "    is_thinking_chunk = isinstance(response, ThinkingChunkEvent)\n",
    "    is_summary = isinstance(response, ResponseSummary)\n",
    "\n",
    "    if is_text_chunk or is_thinking_chunk:\n",
    "        if type(response) is not current_type:\n",
    "            print(f\"\\n\\n[{'THINKING' if is_thinking_chunk else 'TEXT'}]\")\n",
    "            current_type = type(response)\n",
    "        print(response.content, end=\"\")\n",
    "    elif isinstance(response, ResponseSummary):\n",
    "        summary = response\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response type: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReasoningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUESTION]: I'm planning a trip to New York and Miami. What's the weather like in both cities? Also, if I have a budget of $2400 for a 6-day trip, how much can I spend per day?\n",
      "\n",
      "\n",
      "--- Iteration 1\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "I need to gather the current weather information for both New York and Miami. After that, I will calculate the daily budget for the trip based on the total budget of $2400 for 6 days.\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: get_weather`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"location\": \"New York\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: get_weather`\n",
      "Result: Weather for New York: 68°F, Partly Cloudy\n",
      "\n",
      "--- Iteration 2\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "I have obtained the current weather for New York, which is 68°F and partly cloudy. Now, I will gather the weather information for Miami to complete the weather comparison.\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: get_weather`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"location\": \"Miami\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: get_weather`\n",
      "Result: Weather for Miami: 85°F, Sunny\n",
      "\n",
      "--- Iteration 3\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "I have now gathered the weather information for both cities: New York is 68°F and partly cloudy, while Miami is 85°F and sunny. Next, I will calculate the daily budget for the trip. With a total budget of $2400 for 6 days, I can find out how much can be spent per day by dividing the total budget by the number of days.\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: calculator`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"expression\": \"2400 / 6\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: calculator`\n",
      "Result: 400.0\n",
      "\n",
      "--- Iteration 4\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "The daily budget for the trip is $400, calculated by dividing the total budget of $2400 by 6 days. Now I have all the information needed: the weather in New York and Miami, and the daily budget for the trip.\n",
      "\n",
      "[FINAL RESPONSE]:\n",
      "**Final Answer:**\n",
      "\n",
      "The current weather in New York is 68°F and partly cloudy, while in Miami, it is 85°F and sunny. With a budget of $2400 for a 6-day trip, you can spend $400 per day.\n",
      "\n",
      "**Explanation:**\n",
      "The weather information was gathered for both cities, and the daily budget was calculated by dividing the total budget of $2400 by the number of days (6), resulting in $400 per day.\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sik_llms.models_base import (\n",
    "    Tool, Parameter, ThinkingEvent, ToolPredictionEvent, \n",
    "    ToolResultEvent, TextChunkEvent, ErrorEvent, ResponseSummary\n",
    ")\n",
    "from sik_llms.reasoning_agent import ReasoningAgent\n",
    "\n",
    "# Define a calculator tool\n",
    "calculator_tool = Tool(\n",
    "    name=\"calculator\",\n",
    "    description=\"Perform mathematical calculations\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name=\"expression\", \n",
    "            type=\"string\", \n",
    "            required=True, \n",
    "            description=\"The mathematical expression to evaluate (e.g., '2 + 2', '5 * 10')\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a weather tool\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather for a location\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name=\"location\", \n",
    "            type=\"string\", \n",
    "            required=True, \n",
    "            description=\"The name of the city(e.g., 'San Francisco', 'New York', 'London')\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the tool executors\n",
    "async def calculator_executor(expression: str) -> str:\n",
    "    \"\"\"Execute calculator tool.\"\"\"\n",
    "    try:\n",
    "        # Only allow simple arithmetic for safety\n",
    "        allowed_chars = set(\"0123456789+-*/() .\")\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return \"Error: Invalid characters in expression\"\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "async def weather_executor(location: str) -> str:\n",
    "    \"\"\"Mock weather tool - returns fake data.\"\"\"\n",
    "    # Return mock weather data\n",
    "    weather_data = {\n",
    "        \"New York\": \"68°F, Partly Cloudy\",\n",
    "        \"San Francisco\": \"62°F, Foggy\",\n",
    "        \"Miami\": \"85°F, Sunny\",\n",
    "        \"Chicago\": \"55°F, Windy\",\n",
    "        \"Los Angeles\": \"75°F, Clear\"\n",
    "    }\n",
    "\n",
    "    for city in weather_data:  # noqa: PLC0206\n",
    "        if city.lower() in location.lower():\n",
    "            return f\"Weather for {location}: {weather_data[city]}\"\n",
    "\n",
    "    return f\"Weather for {location} not found\"\n",
    "\n",
    "tool_executors = {\n",
    "    \"calculator\": calculator_executor,\n",
    "    \"get_weather\": weather_executor\n",
    "}\n",
    "\n",
    "# Create the reasoning agent\n",
    "agent = ReasoningAgent(\n",
    "    model_name=\"gpt-4o-mini\",  # You can change this to other models\n",
    "    tools=[calculator_tool, weather_tool],\n",
    "    tool_executors=tool_executors,\n",
    "    max_iterations=10,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "question = \"I'm planning a trip to New York and Miami. What's the weather like in both cities? Also, if I have a budget of $2400 for a 6-day trip, how much can I spend per day?\"\n",
    "# Run the agent and collect the results\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "print(f\"[QUESTION]: {question}\\n\")\n",
    "\n",
    "current_iteration = 0\n",
    "\n",
    "async for event in agent.run_async(messages):\n",
    "    if isinstance(event, ThinkingEvent):\n",
    "        if hasattr(event, 'iteration') and event.iteration != current_iteration:\n",
    "            current_iteration = event.iteration\n",
    "            print(f\"\\n--- Iteration {current_iteration}\\n\")\n",
    "        if event.content:\n",
    "            print(f\"\\n[THINKING]:\\n{event.content}\")\n",
    "\n",
    "    elif isinstance(event, ToolPredictionEvent):\n",
    "        print(\"\\n[TOOL PREDICTION]:\")\n",
    "        print(f\"Tool: {event.name}`\")\n",
    "        print(f\"Parameters: \\n```json\\n{json.dumps(event.arguments, indent=2)}\\n```\")\n",
    "\n",
    "    elif isinstance(event, ToolResultEvent):\n",
    "        print(\"\\n[TOOL RESULT]:\")\n",
    "        print(f\"Tool: {event.name}`\")\n",
    "        print(f\"Result: {event.result}\")\n",
    "\n",
    "    elif isinstance(event, ErrorEvent):\n",
    "        print(\"\\n[ERROR]:\")\n",
    "        print(f\"Error: {event.content}\")\n",
    "\n",
    "    elif isinstance(event, TextChunkEvent):\n",
    "        # For streaming text generation\n",
    "        if current_iteration  >= 0:  # Only print once for the header\n",
    "            print(\"\\n[FINAL RESPONSE]:\")\n",
    "            current_iteration = -1  # Set to an impossible iteration to avoid repeating\n",
    "\n",
    "        print(event.content, end=\"\")\n",
    "\n",
    "    elif isinstance(event, ResponseSummary):\n",
    "        # Print nothing here as we've already streamed the response\n",
    "        pass\n",
    "\n",
    "# Just for notebook display cleanliness\n",
    "print(\"\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens: 6299\n",
      "Output Tokens: 493\n",
      "Total Cost: 0.0012406499999999998\n",
      "Duration: 14.24 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input Tokens: {event.input_tokens}\")\n",
    "print(f\"Output Tokens: {event.output_tokens}\")\n",
    "print(f\"Total Cost: {event.total_cost}\")\n",
    "print(f\"Duration: {event.duration_seconds:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
