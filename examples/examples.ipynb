{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanekercheval/repos/sik-llms/src\n"
     ]
    }
   ],
   "source": [
    "%cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.openai.OpenAI at 0x10c1aee40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For \"registered\" clients (via `@Client.register`), the client\n",
    "# can be created with `create_client` by passing in the model name.\n",
    "from sik_llms import create_client\n",
    "\n",
    "client = create_client(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.openai.OpenAI at 0x10c8ab4d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, the client can be directly instantiated\n",
    "from sik_llms import OpenAI\n",
    "client = OpenAI(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sik_llms.anthropic.Anthropic at 0x10ca1d400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, the client can be directly instantiated\n",
    "from sik_llms import Anthropic\n",
    "client = Anthropic(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    temperature=0.1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user', 'content': 'What is the capital of France?'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sik_llms import create_client, user_message, TextChunkEvent\n",
    "\n",
    "client = create_client(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the capital of France?\")\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Synchronously via `__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=14 output_tokens=7 input_cost=2.1e-06 output_cost=4.2e-06 duration_seconds=0.728931188583374 response='The capital of France is Paris.'\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = client(messages=[message])\n",
    "print(response)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Aynchronously via `run_async`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=14 output_tokens=7 input_cost=2.1e-06 output_cost=4.2e-06 duration_seconds=0.5546269416809082 response='The capital of France is Paris.'\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = await client.run_async(messages=[message])\n",
    "print(response)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Asynchronously via `stream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "async for response in client.stream(messages=[message]):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        print(response.content, end=\"\")\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextChunkEvent(content='The', logprob=0.0),\n",
       " TextChunkEvent(content=' capital', logprob=0.0),\n",
       " TextChunkEvent(content=' of', logprob=0.0),\n",
       " TextChunkEvent(content=' France', logprob=0.0),\n",
       " TextChunkEvent(content=' is', logprob=0.0),\n",
       " TextChunkEvent(content=' Paris', logprob=-1.1160349458805285e-05),\n",
       " TextChunkEvent(content='.', logprob=-6.704273118884885e-07),\n",
       " TextResponse(input_tokens=14, output_tokens=7, input_cost=2.1e-06, output_cost=4.2e-06, duration_seconds=0.572174072265625, response='The capital of France is Paris.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseSummary(input_tokens=14, output_tokens=49, input_cost=4.2000000000000004e-05, output_cost=0.000735, duration_seconds=1.1158390045166016, response=\"The capital of France is Paris. It's not only the capital city but also the largest city in France, known for its iconic landmarks like the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = create_client(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    temperature=0.1,\n",
    ")\n",
    "response = client(messages=[user_message(\"What is the capital of France?\")])\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Functions/Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=60 output_tokens=17 input_cost=9e-06 output_cost=1.0199999999999999e-05 duration_seconds=0.7561488151550293 tool_prediction=ToolPrediction(name='get_weather', arguments={'location': 'Paris, France'}, call_id='call_PUeEPiBRUbF3aGFUfq0NyU7u') message=None\n",
      "---\n",
      "name='get_weather' arguments={'location': 'Paris, France'} call_id='call_PUeEPiBRUbF3aGFUfq0NyU7u'\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    Tool, Parameter, RegisteredClients,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name='get_weather',\n",
    "    description=\"Get the weather for a location.\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='location',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description='The city and country for weather info.',\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    client_type=RegisteredClients.OPENAI_TOOLS,\n",
    "    model_name='gpt-4o-mini',\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the weather in Paris?\")\n",
    "response = await client.run_async(messages=[message])\n",
    "# or `response = client(messages=[message])` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.tool_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude Functions/Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=401 output_tokens=40 input_cost=0.001203 output_cost=0.0006000000000000001 duration_seconds=1.515820026397705 tool_prediction=ToolPrediction(name='get_weather', arguments={'location': 'Paris, France'}, call_id='toolu_01PNN6At5XHFmYTerwTabuh5') message=None\n",
      "---\n",
      "name='get_weather' arguments={'location': 'Paris, France'} call_id='toolu_01PNN6At5XHFmYTerwTabuh5'\n"
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    Tool, Parameter, RegisteredClients,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name='get_weather',\n",
    "    description=\"Get the weather for a location.\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='location',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description='The city and country for weather info.',\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    client_type=RegisteredClients.ANTHROPIC_TOOLS,\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    tools=[weather_tool],\n",
    ")\n",
    "\n",
    "message = user_message(\"What is the weather in Paris?\")\n",
    "response = await client.run_async(messages=[message])\n",
    "# or `response = client(messages=[message])` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.tool_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs via OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=92 output_tokens=18 input_cost=1.38e-05 output_cost=1.08e-05 duration_seconds=0.7941067218780518 parsed=CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob']) refusal=None\n",
      "---\n",
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):  # noqa: D101\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "client = create_client(\n",
    "    model_name='gpt-4o-mini',\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "messages=[\n",
    "    system_message(\"Extract the event information.\"),\n",
    "    user_message(\"Alice and Bob are going to a science fair on Friday.\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "# or `response = client(messages=messages)` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs via Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens=441 output_tokens=78 input_cost=0.001323 output_cost=0.00117 duration_seconds=2.1678481101989746 parsed=CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob']) refusal=None\n",
      "---\n",
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):  # noqa: D101\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "client = create_client(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "messages=[\n",
    "    system_message(\"Extract the event information.\"),\n",
    "    user_message(\"Alice and Bob are going to a science fair on Friday.\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "# or `response = client(messages=messages)` for synchronous execution\n",
    "print(response)\n",
    "print('---')\n",
    "print(response.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning via OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down step by step:\n",
      "\n",
      "1. Calculate inside the parentheses:\n",
      "   - 3 * 4 = 12\n",
      "   - 5 * 6 = 30\n",
      "\n",
      "2. Replace the original expression with these values:\n",
      "   1 + 2 + 12 + 30\n",
      "\n",
      "3. Add them together:\n",
      "   1 + 2 = 3\n",
      "   3 + 12 = 15\n",
      "   15 + 30 = 45\n",
      "\n",
      "So, the answer is 45."
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    TextChunkEvent, TextResponse, ReasoningEffort,\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    model_name='o3-mini',\n",
    "    reasoning_effort=ReasoningEffort.MEDIUM,\n",
    ")\n",
    "messages=[user_message(\"What is 1 + 2 + (3 * 4) + (5 * 6)?\")]\n",
    "summary = None\n",
    "async for response in client.stream(messages=messages):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        print(response.content, end=\"\")\n",
    "    elif isinstance(response, TextResponse):\n",
    "        summary = response\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response type: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseSummary(input_tokens=28, output_tokens=104, input_cost=3.08e-05, output_cost=0.0004576, duration_seconds=2.3381471633911133, response=\"Let's break it down step by step:\\n\\n1. Calculate inside the parentheses:\\n   - 3 * 4 = 12\\n   - 5 * 6 = 30\\n\\n2. Replace the original expression with these values:\\n   1 + 2 + 12 + 30\\n\\n3. Add them together:\\n   1 + 2 = 3\\n   3 + 12 = 15\\n   15 + 30 = 45\\n\\nSo, the answer is 45.\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning via Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[THINKING]\n",
      "Let me solve this step by step, following the order of operations (PEMDAS).\n",
      "\n",
      "First, let's calculate the expressions in parentheses:\n",
      "- (3 * 4) = 12\n",
      "- (5 * 6) = 30\n",
      "\n",
      "Now I can rewrite the expression as:\n",
      "1 + 2 + 12 + 30\n",
      "\n",
      "Now I'll add these numbers:\n",
      "1 + 2 = 3\n",
      "3 + 12 = 15\n",
      "15 + 30 = 45\n",
      "\n",
      "So the final answer is 45.\n",
      "\n",
      "[TEXT]\n",
      "To solve this expression, I'll use the order of operations (PEMDAS):\n",
      "\n",
      "First, I'll evaluate the expressions in parentheses:\n",
      "- (3 * 4) = 12\n",
      "- (5 * 6) = 30\n",
      "\n",
      "Now I can rewrite the expression as:\n",
      "1 + 2 + 12 + 30\n",
      "\n",
      "Adding these numbers:\n",
      "1 + 2 + 12 + 30 = 45\n",
      "\n",
      "The answer is 45."
     ]
    }
   ],
   "source": [
    "from sik_llms import (\n",
    "    create_client, user_message,\n",
    "    TextChunkEvent, ThinkingChunkEvent,\n",
    "    TextResponse, ReasoningEffort,\n",
    ")\n",
    "\n",
    "client = create_client(\n",
    "    model_name='claude-3-7-sonnet-latest',\n",
    "    reasoning_effort=ReasoningEffort.MEDIUM,\n",
    ")\n",
    "messages=[user_message(\"What is 1 + 2 + (3 * 4) + (5 * 6)?\")]\n",
    "summary = None\n",
    "\n",
    "current_type = None\n",
    "async for response in client.stream(messages=messages):\n",
    "    is_text_chunk = isinstance(response, TextChunkEvent)\n",
    "    is_thinking_chunk = isinstance(response, ThinkingChunkEvent)\n",
    "    is_summary = isinstance(response, TextResponse)\n",
    "\n",
    "    if is_text_chunk or is_thinking_chunk:\n",
    "        if type(response) is not current_type:\n",
    "            print(f\"\\n\\n[{'THINKING' if is_thinking_chunk else 'TEXT'}]\")\n",
    "            current_type = type(response)\n",
    "        print(response.content, end=\"\")\n",
    "    elif isinstance(response, TextResponse):\n",
    "        summary = response\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response type: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseSummary(input_tokens=60, output_tokens=248, input_cost=0.00018, output_cost=0.00372, duration_seconds=4.046967029571533, response=\"Let me solve this step by step, following the order of operations (PEMDAS).\\n\\nFirst, let's calculate the expressions in parentheses:\\n- (3 * 4) = 12\\n- (5 * 6) = 30\\n\\nNow I can rewrite the expression as:\\n1 + 2 + 12 + 30\\n\\nNow I'll add these numbers:\\n1 + 2 = 3\\n3 + 12 = 15\\n15 + 30 = 45\\n\\nSo the final answer is 45.To solve this expression, I'll use the order of operations (PEMDAS):\\n\\nFirst, I'll evaluate the expressions in parentheses:\\n- (3 * 4) = 12\\n- (5 * 6) = 30\\n\\nNow I can rewrite the expression as:\\n1 + 2 + 12 + 30\\n\\nAdding these numbers:\\n1 + 2 + 12 + 30 = 45\\n\\nThe answer is 45.\")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReasoningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUESTION]: I'm planning a trip to New York and Miami. What's the weather like in both cities? Also, if I have a budget of $2400 for a 6-day trip, how much can I spend per day?\n",
      "\n",
      "\n",
      "--- Iteration 1\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "To provide the weather information for New York and Miami, I need to gather the current weather data for both cities. Additionally, I need to calculate the daily budget for a 6-day trip with a total budget of $2400. First, I will calculate the daily budget, and then I will check the weather for both cities.\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: get_weather`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"location\": \"New York\",\n",
      "  \"units\": \"F\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: get_weather`\n",
      "Result: {'New York': '68°F'}\n",
      "\n",
      "--- Iteration 2\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "I have obtained the current weather for New York, which is 68°F. Now, I will proceed to get the weather for Miami. After that, I will calculate the daily budget for the trip.\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: get_weather`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"location\": \"Miami\",\n",
      "  \"units\": \"F\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: get_weather`\n",
      "Result: {'Miami': '85°F'}\n",
      "\n",
      "--- Iteration 3\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "I have now obtained the current weather for both cities: New York is 68°F and Miami is 85°F. Next, I will calculate the daily budget for the trip. With a total budget of $2400 for a 6-day trip, I can find out how much can be spent per day by dividing the total budget by the number of days.\n",
      "\n",
      "[TOOL PREDICTION]:\n",
      "Tool: calculator`\n",
      "Parameters: \n",
      "```json\n",
      "{\n",
      "  \"expression\": \"2400 / 6\"\n",
      "}\n",
      "```\n",
      "\n",
      "[TOOL RESULT]:\n",
      "Tool: calculator`\n",
      "Result: 400.0\n",
      "\n",
      "--- Iteration 4\n",
      "\n",
      "\n",
      "[THINKING]:\n",
      "The daily budget for the trip is $400. I now have all the information needed to provide a complete answer regarding the weather in both cities and the daily budget for the trip.\n",
      "\n",
      "[FINAL RESPONSE]:\n",
      "Based on the reasoning process, here is the final answer:\n",
      "\n",
      "**Weather:**\n",
      "- New York: 68°F\n",
      "- Miami: 85°F\n",
      "\n",
      "**Budget:**\n",
      "With a total budget of $2400 for a 6-day trip, you can spend **$400 per day**.\n",
      "\n",
      "**Explanation:**\n",
      "1. The weather data was retrieved for both cities, showing that New York is currently 68°F and Miami is 85°F.\n",
      "2. The daily budget was calculated by dividing the total budget of $2400 by the number of days (6), resulting in a daily spending limit of $400. \n",
      "\n",
      "This information should help you plan your trip effectively!\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sik_llms.models_base import (\n",
    "    Tool, Parameter, ThinkingEvent, ToolPredictionEvent,\n",
    "    ToolResultEvent, TextChunkEvent, ErrorEvent, TextResponse,\n",
    ")\n",
    "from sik_llms.reasoning_agent import ReasoningAgent\n",
    "\n",
    "####\n",
    "# Define the tool functions\n",
    "####\n",
    "async def calculator(expression: str) -> str:\n",
    "    \"\"\"Execute calculator tool.\"\"\"\n",
    "    try:\n",
    "        # Only allow simple arithmetic for safety\n",
    "        allowed_chars = set('0123456789+-*/() .')\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return \"Error: Invalid characters in expression\"\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e!s}\"\n",
    "\n",
    "\n",
    "async def weather(location: str, units: str) -> str:\n",
    "    \"\"\"Mock weather tool - returns fake data.\"\"\"\n",
    "    # Return mock weather data\n",
    "    weather_data = {\n",
    "        'New York': '68',\n",
    "        'San Francisco': '62',\n",
    "        'Miami': '85',\n",
    "        'Chicago': '55',\n",
    "        'Los Angeles': '75',\n",
    "    }\n",
    "    for city in weather_data:  # noqa: PLC0206\n",
    "        if city.lower() in location.lower():\n",
    "            temp = weather_data[city]\n",
    "            if units == 'C':\n",
    "                # C = (°F - 32) x (5/9)\n",
    "                temp = round((temp - 32) * 5 / 9)\n",
    "            return {location: f\"{temp}°{units}\"}\n",
    "    return None\n",
    "\n",
    "####\n",
    "# Define tool objects\n",
    "####\n",
    "calculator_tool = Tool(\n",
    "    name='calculator',\n",
    "    description=\"Perform mathematical calculations\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name='expression',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description=\"The mathematical expression to evaluate (e.g., '2 + 2', '5 * 10')\",\n",
    "        ),\n",
    "    ],\n",
    "    func=calculator,\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather for a location\",\n",
    "    parameters=[\n",
    "        Parameter(\n",
    "            name=\"location\",\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description=\"The name of the city (e.g., 'San Francisco', 'New York', 'London')\",\n",
    "        ),\n",
    "        Parameter(\n",
    "            name='units',\n",
    "            param_type=str,\n",
    "            required=True,\n",
    "            description=\"The units for temperature\",\n",
    "            valid_values=['F', 'C'],\n",
    "        ),\n",
    "    ],\n",
    "    func=weather,\n",
    ")\n",
    "\n",
    "# Create the reasoning agent\n",
    "agent = ReasoningAgent(\n",
    "    model_name=\"gpt-4o-mini\",  # You can change this to other models\n",
    "    tools=[calculator_tool, weather_tool],\n",
    "    max_iterations=10,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "question = \"I'm planning a trip to New York and Miami. What's the weather like in both cities? Also, if I have a budget of $2400 for a 6-day trip, how much can I spend per day?\"  # noqa: E501\n",
    "# Run the agent and collect the results\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "print(f\"[QUESTION]: {question}\\n\")\n",
    "\n",
    "current_iteration = 0\n",
    "\n",
    "async for event in agent.stream(messages):\n",
    "    if isinstance(event, ThinkingEvent):\n",
    "        if hasattr(event, 'iteration') and event.iteration != current_iteration:\n",
    "            current_iteration = event.iteration\n",
    "            print(f\"\\n--- Iteration {current_iteration}\\n\")\n",
    "        if event.content:\n",
    "            print(f\"\\n[THINKING]:\\n{event.content}\")\n",
    "\n",
    "    elif isinstance(event, ToolPredictionEvent):\n",
    "        print(\"\\n[TOOL PREDICTION]:\")\n",
    "        print(f\"Tool: {event.name}`\")\n",
    "        print(f\"Parameters: \\n```json\\n{json.dumps(event.arguments, indent=2)}\\n```\")\n",
    "\n",
    "    elif isinstance(event, ToolResultEvent):\n",
    "        print(\"\\n[TOOL RESULT]:\")\n",
    "        print(f\"Tool: {event.name}`\")\n",
    "        print(f\"Result: {event.result}\")\n",
    "\n",
    "    elif isinstance(event, ErrorEvent):\n",
    "        print(\"\\n[ERROR]:\")\n",
    "        print(f\"Error: {event.content}\")\n",
    "\n",
    "    elif isinstance(event, TextChunkEvent):\n",
    "        # For streaming text generation\n",
    "        if current_iteration  >= 0:  # Only print once for the header\n",
    "            print(\"\\n[FINAL RESPONSE]:\")\n",
    "            current_iteration = -1  # Set to an impossible iteration to avoid repeating\n",
    "\n",
    "        print(event.content, end=\"\")\n",
    "\n",
    "    elif isinstance(event, TextResponse):\n",
    "        # Print nothing here as we've already streamed the response\n",
    "        pass\n",
    "\n",
    "# Just for notebook display cleanliness\n",
    "print(\"\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens: 4924\n",
      "Output Tokens: 512\n",
      "Total Cost: 0.0010458\n",
      "Duration: 11.69 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input Tokens: {event.input_tokens}\")\n",
    "print(f\"Output Tokens: {event.output_tokens}\")\n",
    "print(f\"Total Cost: {event.total_cost}\")\n",
    "print(f\"Duration: {event.duration_seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Difference tell behind travel attorney. Second provide eye term wife man. Business nothing person my stop listen.\\nAct happy many imagine arm quality drug. Wear yard answer morning rest think art natio'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faker import Faker\n",
    "\n",
    "cache_content = Faker().text(max_nb_chars=15_000)\n",
    "cache_content[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first word of the cached text is \"Difference\".\n",
      "---\n",
      "Total Cost: 0.0029365999999999997\n",
      "---\n",
      "Input Tokens: 17\n",
      "Output Tokens: 15\n",
      "Cache Write Tokens: 2863\n",
      "Cache Read Tokens: 0\n",
      "Total Tokens: 2895\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This example is modified from anthropic's prompt-caching.ipynb notebook\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "####\n",
    "from sik_llms import Anthropic\n",
    "from sik_llms.models_base import system_message, user_message\n",
    "\n",
    "client = Anthropic(\n",
    "    model_name='claude-3-5-haiku-latest',\n",
    "    temperature=0.1,\n",
    ")\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb\n",
    "system_messages = [\n",
    "    system_message(\"You are a helpful assistant.\"),\n",
    "    system_message(\n",
    "        cache_content,\n",
    "        cache_control={'type': 'ephemeral'},\n",
    "    ),\n",
    "]\n",
    "messages = [\n",
    "    *system_messages,\n",
    "    user_message(\"What is the first word of the cached text?\"),\n",
    "]\n",
    "\n",
    "# first run should result in a cache-miss & write\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second word of the cached text is \"tell\".\n",
      "---\n",
      "Total Cost: 0.00029864000000000003\n",
      "---\n",
      "Input Tokens: 17\n",
      "Output Tokens: 14\n",
      "Cache Write Tokens: 0\n",
      "Cache Read Tokens: 2863\n",
      "Total Tokens: 2894\n"
     ]
    }
   ],
   "source": [
    "# second run should result in a cache-hit & read\n",
    "messages = [\n",
    "    *system_messages,\n",
    "    user_message(\"What is the second word of the cached text?\"),\n",
    "]\n",
    "response = await client.run_async(messages=messages)\n",
    "print(response.response)\n",
    "print('---')\n",
    "print(f\"Total Cost: {response.total_cost}\")\n",
    "print('---')\n",
    "print(f\"Input Tokens: {response.input_tokens}\")\n",
    "print(f\"Output Tokens: {response.output_tokens}\")\n",
    "print(f\"Cache Write Tokens: {response.cache_write_tokens}\")\n",
    "print(f\"Cache Read Tokens: {response.cache_read_tokens}\")\n",
    "print(f\"Total Tokens: {response.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock via OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sik_llms import OpenAI, user_message\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    server_url=os.getenv('BEDROCK_API_URL'),\n",
    "    api_key=os.getenv('BEDROCK_API_KEY'),\n",
    "    model_name='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "    user='bedrock-requires-user?',\n",
    ")\n",
    "response = client(messages=[user_message(\"What is the capital of France?\")])\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Log Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wh (logprob: -1.76, prob: 0.17)\n",
      "ispers (logprob: -0.00, prob: 1.00)\n",
      " of (logprob: -0.10, prob: 0.91)\n",
      " the (logprob: -0.00, prob: 1.00)\n",
      " tide (logprob: -0.38, prob: 0.68)\n",
      ", (logprob: -0.00, prob: 1.00)\n",
      "  \n",
      " (logprob: -0.00, prob: 1.00)\n",
      "S (logprob: -7.07, prob: 0.00)\n",
      "apphire (logprob: -2.21, prob: 0.11)\n",
      " waves (logprob: -0.08, prob: 0.93)\n",
      " kiss (logprob: -0.38, prob: 0.68)\n",
      " sandy (logprob: -1.69, prob: 0.18)\n",
      " shore (logprob: -2.58, prob: 0.08)\n",
      ", (logprob: -0.01, prob: 0.99)\n",
      "  \n",
      " (logprob: -0.00, prob: 1.00)\n",
      "Secrets (logprob: -0.49, prob: 0.61)\n",
      " deep (logprob: -1.15, prob: 0.32)\n",
      " reside (logprob: -0.60, prob: 0.55)\n",
      ". (logprob: -0.00, prob: 1.00)\n",
      "   (logprob: -1.70, prob: 0.18)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from dotenv import load_dotenv\n",
    "from sik_llms import OpenAI, user_message, TextChunkEvent\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    model_name='gpt-4o-mini',\n",
    "    logprobs=True,\n",
    ")\n",
    "messages = [\n",
    "    user_message(\"Write a haiku about the ocean.\"),\n",
    "]\n",
    "async for response in client.stream(messages=messages):\n",
    "    if isinstance(response, TextChunkEvent):\n",
    "        log_prob = response.logprob\n",
    "        prob = math.exp(log_prob)\n",
    "        print(f\"{response.content} (logprob: {log_prob:.2f}, prob: {prob:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
